\section{Experiments on Imbalanced Image Classification}
% \begin{table}[ht]
%     \centering
%     \footnotesizer
%     \caption{Efficiency comparison between input space DA with generative models and GeLDA. NP refers to the number of parameters.} % with 24 GB of memory.}
%     \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{crrcrc}
%         \toprule 
%         \textbf{Model} & \textbf{Training time} & \textbf{Training data} & \textbf{Inference time} & \textbf{NP} \\
%         % & \textbf{Data size}\\
%         \midrule
%         % XTTS & - & 27.3K h & 380 ms/sec & 466.9 M & \\%242.0 KB/sample\\
%         Seamless M4T & N/A & 4,500,000 h & 270 ms/sec& 2,300 M \\
%         % & 31.25 KB/sec\\
%         %118.7 KB/sample\\
%         Seamless Expressive & N/A & 29,800 h & 270 ms/sec& 2,100 M \\
%         % & \\%92.2 KB/sample\\
%         \hline
%         GeLDA (Ours) & AA h& $\approx$ 82 h & 550 ms/utterance& \todo{21} M 
%         \\
%         % & 8.6 KB/utterance\\
%         % Proposed ($\mathcal{Z}^{(1)}$) & ~ & $\approx$76.9 h\\
%         % Proposed ($\mathcal{Z}^{(2)}$) & ~ & $\approx$76.9 h\\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:compare2}
% \end{table}


\subsection{Task Definition and Dataset}

We also demonstrate the effectiveness of GeLDA on the imbalanced image classification task.
We use the ImageNet-LT~\cite{imagenetlt} and Places365~\cite{places365} datasets. ImageNet-LT is a long-tailed image classification dataset derived from ImageNet-2012~\cite{imagenet_orig}, consisting of 1,000 categories with 186.3K images. Places365 is a large-scale dataset for place recognition and scene understanding, consisting of 365 categories with 1.7M\todo{check the number of sampels for Places365} images.
% The training split includes 115.8K images from 5 to 1,280 images per class, while the balanced test split follows the original ImageNet.
Performance is reported on many ($>100$), medium ($20–100$), and small ($<20$) class splits to reflect the challenges of long-tailed learning. Our goal is to achieve high accuracy in the small subset while preserving the performance of the many and medium subsets.
% In this setup, the training dataset $\mathcal{S}$ consists of three subset of labeled image dataset depend on the number of their images per label; head, medium, tail subsets. 
% $D_i=\{(x_n, y_n) \mid n=1,2,\ldots,N_i\}$ where $x$ and $y$ are image and 

\subsection{Experiment setup}
For the baseline model, we adopt the LIFT~\cite{lift}, which utilizes CLIP~\cite{clip} ViT-B/16 as a backbone and a state-of-the-art model in these datasets. To follow the architecture, the task adapter here is changed to a simple linear layer. For the diffusion model conditioning, we use latent vectors both from image and text embeddings $z_\text{img}$ and $z_\text{txt}$, that share the same target image label $y$.  Then, we generated 100 samples for each small class. This ensures that each class has at least 100 examples. Besides these, all other experiment settings are the same as those of the multilingual SER task. 

For comparison, we additionally adopt Stable Diffusion 3 (SD3)~\cite{sd3}, a state-of-the-art image generation model, for input-space data augmentation. We first generate images using the text prompt “this is a picture of [label]”, and refer to this setting as SD3. To generate more diverse images, we further employ Qwen~\cite{qwen}, a large language model (LLM), to produce diverse text prompts for each given label. Images generated using these prompts are denoted as SD3-Qwen. The detailed prompts are provided in Appendix~\ref{}.

% CLIP~\cite{clip} shows remarkable zero-shot image classification performance~\cite{clip_vqa}, and 
% We first report the zero-shot performance of CLIP as a reference. For the baseline model, we train the task adopter using the ImageNetLT dataset. Previous works on imbalanced image classification tasks can be broadly categorized into three groups: (1) Training from scratch, which uses the ViT-B architecture without leveraging pre-trained weights~\cite{learning_imbalanced_data_with_vision_transformers, DeiT-LT}; (2) Fine-tuning pre-trained weights, which starts with pre-trained ViT-B weights and updates the entire model during fine-tuning~\cite{ballad, The_equalization_loss}; and (3) Adapter-based fine-tuning, which keeps most of the pre-trained ViT-B frozen and introduces additional parameters (adapters) or updates a small part of them during fine-tuning~\cite{lift}. In contrast, our approach utilizes the pre-trained weights of the foundation model without modifying or updating the model itself during adaptation. For evaluation, we adopt the UA and WA metrics, consistent with the SER task.


% First, share the ViT-B architecture, but do not use the pre-trained weights and train it from scratch~\cite{learning_imbalanced_data_with_vision_transformers, DeiT-LT}. Second, use the pre-trained weights and update the ViT-B during fine-tuning~\cite{ballad, The_equalization_loss}. Last, use the pre-trained weights, but in the fine-tuned stage, adopt additional weights aside the ViT-B layer~\cite{lift}. However, our method aims to use the pre-trained weights of the foundation model, but assumes that we do not need to modify the foundation model at all. For the evaluation metrics, we also adopted the same evaluation metrics, UA and WA, from the SER task.


% \begin{table}[t]
% \centering
% \caption{Performance on ImageNetLT.}
% \begin{tabular}{lc|rrr}
% \toprule
% Method & Overall & Many & Med & Small \\
% \midrule
% LiVT & 60.9 & 73.6 & 56.4 & 41.0  \\
% DeiT-LT & 59.1 & 66.6 & 58.3 & 40.0 \\
% BALLAD & 75.7 & 79.1 & 74.5 & 69.8 \\
% VL-LTR & 77.2 & 84.5 & 74.6 & 59.3 \\
% % LIFT+ & 77.0 & 79.9 & 76.1 & 71.7 \\
% % + TTE & 78.3 & 81.0 & 77.5 & 73.4 \\
% LIFT & 77.0 & 80.2 & 76.1 & 71.5 \\
% + TTE & \textbf{78.3} & 81.2 & 77.4 & 73.4 \\
% \midrule
% Stable Diffusion 3 & 76.8 & 80.5 & 76.5 & 67.6 \\
% + TTE & 78.0 & \textbf{81.4} & \textbf{77.7} & 69.2 \\
% SD3 Qwen & 76.8 & 80.4 & 76.4 & 68.0 \\
%  + TTE & 77.9 & 81.3 & 77.7 & 69.2 \\
% GeLDA (Ours) & 77.0 & 79.7 & 75.7 & 73.9 \\
% + TTE & 78.2 & 80.8 & 77.1 & \textbf{74.7} \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[t]
\centering
\caption{Performance (\%) on ImageNet-LT. 
$\mathcal{X}$ denotes input-space augmentation and $\mathcal{Z}$ denotes latent-space augmentation.}
\label{tab:imagenetlt}
\small
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.1}

\resizebox{\linewidth}{!}{
\begin{tabular}{lcc|ccc}
\toprule
\textbf{Method} & \textbf{Aug. Space} & \textbf{Overall} & \textbf{Many} & \textbf{Medium} & \textbf{Small} \\
\midrule
LiVT        & -- & 60.9 & 73.6 & 56.4 & 41.0 \\
DeiT-LT     & -- & 59.1 & 66.6 & 58.3 & 40.0 \\
BALLAD      & -- & 75.7 & 79.1 & 74.5 & 69.8 \\
VL-LTR      & -- & 77.2 & 84.5 & 74.6 & 59.3 \\
\hline
LIFT        & -- & 77.0 & 80.2 & 76.1 & 71.5 \\
+ TTE       & -- & 78.3 & 81.2 & 77.4 & 73.4 \\
\hline
\multicolumn{6}{l}{\textit{Augmentation on top of LIFT}} \\
\hline
SD3                  & \multirow{2}{*}{$\mathcal{X}$} & 76.8 & 80.5 & 76.5 & 67.6 \\
+ TTE            & ~ & 78.0 & 81.4 & 77.7 & 69.2 \\
SD3 Qwen             & \multirow{2}{*}{$\mathcal{X}$} & 76.8 & 80.4 & 76.4 & 68.0 \\
+ TTE       & ~ & 77.9 & 81.3 & 77.7 & 69.2 \\
\hline
GeLDA (Ours)         & \multirow{2}{*}{$\mathcal{Z}$} & 77.0 & 79.7 & 75.7 & 73.9 \\
+ TTE   & ~ & 78.2 & 80.8 & 77.1 & \textbf{74.7} \\
\bottomrule
\end{tabular}
}

\end{table}





% \begin{table}[t]
% \centering
% \caption{Performance on Places365.}
% \begin{tabular}{lr|rrr}
% \toprule
% Method & Overall & Many & Med & Small \\
% \midrule
% LiVT & 40.8 & 48.1 & 40.6 & 27.5 \\
% BALLAD & 49.5 & 49.3 & 50.2 & 48.4 \\
% VL-LTR & 50.1 & 54.2 & 48.5 & 42.0 \\
% % LIFT+ & 51.5 & 50.8 & 52.0 & 51.6 \\
% % + TTE & 52.1 & 51.5 & 52.8 & 51.7 \\
% LIFT & 51.5 & 51.3 & 52.2 & 50.4 \\
% +TTE & \textbf{52.2} & 51.7 & \textbf{53.1} & 50.9 \\
% \midrule
% Stable Diffusion 3 & 50.9 & 51.8 & 52.1 & 46.3 \\
% +TTE & 51.6 & \textbf{52.0} & 53.0 & 47.4 \\
% SD3 Qwen & 50.8 & 51.5 & 52.5 & 45.6 \\
%  + TTE & 51.5 & 51.7 & 53.3 & 47.0 \\
% GeLDA (Ours) & 51.5 & 50.6 & 51.8 & \textbf{52.5} \\
% +TTE & 52.1 & 51.4 & 52.6 & 52.4 \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[t]
\centering
\caption{Performance (\%) on Places365. $\mathcal{X}$ and $\mathcal{Z}$ denote input-space and latent-space augmentation, respectively.}
\label{tab:places365}
\small
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.1}
\resizebox{\linewidth}{!}{
\begin{tabular}{lcc|ccc}
\toprule
\textbf{Method} & \textbf{Aug. Space} & \textbf{Overall} & \textbf{Many} & \textbf{Medium} & \textbf{Small} \\
\midrule
LiVT        & -- & 40.8 & 48.1 & 40.6 & 27.5 \\
BALLAD      & -- & 49.5 & 49.3 & 50.2 & 48.4 \\
VL-LTR      & -- & 50.1 & 54.2 & 48.5 & 42.0 \\
\hline
LIFT        & -- & 51.5 & 51.3 & 52.2 & 50.4 \\
+ TTE  & -- & 52.2 & 51.7 & 53.1 & 50.9 \\
\hline
\multicolumn{6}{l}{\textit{Augmentation on top of LIFT}} \\
\hline
SD3                 & \multirow{2}{*}{$\mathcal{X}$} & 50.9 & 51.8 & 52.1 & 46.3 \\
+ TTE           & ~ & 51.6 & 52.0 & 53.0 & 47.4 \\
SD3 Qwen            & \multirow{2}{*}{$\mathcal{X}$} & 50.8 & 51.5 & 52.5 & 45.6 \\
+ TTE      & ~ & 51.5 & 51.7 & 53.3 & 47.0 \\
\hline
GeLDA (Ours)        & \multirow{2}{*}{$\mathcal{Z}$}& 51.5 & 50.6 & 51.8 & \textbf{52.5} \\
+ TTE  & ~ & 52.1 & 51.4 & 52.6 & 52.4 \\
\bottomrule
\end{tabular}
}
\end{table}


% \begin{table}[t]
% \centering
%     \footnotesize
%     \caption{Results on the ImageNet-LT dataset, with all models using ViT-B/16 as the backbone. A circle in the \textbf{pre-train} column indicates the use of CLIP pre-trained weights. A circle in the \textbf{full fine-tune} column indicates that the model fine-tunes the backbone. A triangle indicates the use of an additional adaptation block to the foundation model. 
%     % In contrast, our method requires no access to the foundation model.
%     }
%     \centering
%     \footnotesize
%     \resizebox{0.9\textwidth}{!}{
%     \begin{tabular}{lccccccrr}
%         \toprule
%         \multicolumn{1}{c}{\textbf{Model}} & 
%         \multicolumn{1}{c}{\textbf{DA}} & \multicolumn{1}{c}{\textbf{Pre-train}} & \multicolumn{1}{c}{\textbf{Full fine-tune}} & \multicolumn{1}{c}{\textbf{Head} (\%)} & \multicolumn{1}{c}{\textbf{Medium} (\%)} & \multicolumn{1}{c}{\textbf{Tail} (\%)} & 
%         \multicolumn{1}{c}{\textbf{UA} (\%)} &
%         \multicolumn{1}{c}{\textbf{WA} (\%)}\\
%         \midrule
%         % ViT~\cite{vit} & - & $\times$ & $\times$ & 50.5 & 23.5 & 6.9 & 27.0 & 31.6 \\
% LiVT~\cite{learning_imbalanced_data_with_vision_transformers} & - & $\times$  & $\times$ & 76.4 & 69.7 & 42.7 & 62.9& 63.8 \\
% DeiT-LT \cite{DeiT-LT} & - & $\times$  & $\times$ & 66.6 & 58.3 & 40.0 & 55.0 & 59.1 \\
% LIFT~\cite{lift} & - & $\bigcirc$ & $\triangle$ & 80.2 & 76.1 & 71.5 & 75.9 & 77.0 \\
% BALLAD~\cite{ballad} & - & $\bigcirc$ & $\bigcirc$ & 79.1 & 74.5 & 69.8 & 74.5 & 75.7 \\
% EQL~\cite{The_equalization_loss} & - & $\bigcirc$ & $\bigcirc$ & 79.7 & 71.7 & 67.1 & 72.8 & 74.2 \\
% \hline
% Zero-shot \cite{clip} & - & $\bigcirc$ & $\times$ & 65.3 & 
%         62.8 & 63.7 & 63.9 & 63.9\\
% Baseline & - & $\bigcirc$ & $\times$ & 76.1 & 68.7 & 51.8 & 65.5 & 69.1 \\
% \hline
% ~ & $\mathcal{Z}^{(0)}$ & $\bigcirc$ & $\times$ & 76.0 & 70.2 & 59.6 & 68.6 & 70.9 \\
% GeLDA (Ours) &  $\mathcal{Z}^{(1)}$ & $\bigcirc$ & $\times$ & 75.8 & 70.1 & 58.2 & 68.0 & 70.6 \\
% ~ &  $\mathcal{Z}^{(2)}$ & $\bigcirc$ & $\times$ & 75.5 & 69.7 & 59.9 & 68.4 & 70.5 \\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:result-image}
% \end{table}


\begin{table}[t]
\centering
\caption{Performance on ImageNetLT.}
\label{tab:imagenetlt-zero-shot}
\begingroup
\setlength{\tabcolsep}{3pt}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lc|ccccccc}
\toprule
\textbf{Method} & \textbf{Overall} & \textbf{Many} & \textbf{Med} & \textbf{Small} & \textbf{5-shot} & \textbf{3-shot} & \textbf{1-shot} & \textbf{0-shot} \\
\midrule
LIFT & 76.5 & 80.3 & 76.2 & 66.9 & 54.7 & 44.3 & 26.2 & 8.3 \\
+ TTE       &77.8 & 81.3 & 77.6 & 68.7 & 56.3 & 47.4 & 26.7 & 10.2 \\
\hline
\multicolumn{9}{l}{\textit{Augmentation on top of LIFT}} \\
\hline
SD3 & 76.6 & 80.5 & 76.5 & 66.1 & 57.2 & 55.2 & 54.8 & 41.4 \\
+ TTE       &77.9 & \textbf{81.5} & \textbf{77.9} & 67.4 & 59.0 & 58.8 & 56.2 & 42.6 \\
SD3 Qwen & 76.6 & 80.3 & 76.5 & 66.3 & 54.3 & 56.2 & 51.4 & 42.5 \\
+TTE & 77.8 & 81.4 & 77.9 & 67.5 & 56.6 & 59.2 & 52.2 & 43.8 \\
\hline
GeLDA (Ours) & 76.9 & 79.8 & 75.6 & 72.7 & 62.9 & 67.4 & 65.0 & 56.2 \\
+ TTE       & \textbf{78.1} & 80.9 & 77.1 & \textbf{73.9} & \textbf{65.2} & \textbf{69.7} & \textbf{65.9} & \textbf{57.5} \\
\bottomrule
\end{tabular}
}
\endgroup
\end{table}

\begin{table}[t]
\centering
\caption{Performance on Places365.}
\label{tab:places365-zero-shot}
\begingroup
\setlength{\tabcolsep}{3pt}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lc|ccccccc}
\toprule
\textbf{Method} & \textbf{Overall} & \textbf{Many} & \textbf{Med} & \textbf{Small} & \textbf{5-shot} & \textbf{3-shot} & \textbf{1-shot} & \textbf{0-shot} \\
\midrule
LIFT & 50.3 & 51.4 & 52.5 & 43.2 & 46.0 & 25.4 & 11.0 & 3.2 \\
+TTE & 51.0 & 52.0 & \textbf{53.4} & 43.7 & 46.7 & 26.3 & 11.5 & 3.4 \\
\hline
\multicolumn{9}{l}{\textit{Augmentation on top of LIFT}} \\
\hline
SD3 & 50.4 & 51.6 & 52.4 & 43.7 & 44.8 & 36.0 & 36.4 & 27.6 \\
+TTE               & 51.2 & \textbf{52.0} & 53.3 
& 44.8 & 46.8 & 37.5 & 37.4 & 28.8 \\
SD3 Qwen & 50.3 & 51.3 & 52.7 & 42.8 & 42.3 & 34.8 & 34.2 & 26.3 \\
+TTE & 51.1 & 51.7 & 53.6 & 44.4 & 44.4 & 36.4 & 35.8 & 27.9 \\
\hline
GeLDA (Ours) & 50.9 & 50.3 & 51.6 & \textbf{50.6} & \textbf{52.8} & 44.9 & \textbf{48.1} & \textbf{35.5} \\
+TTE         & \textbf{51.7} & 51.3 & 52.6 & 50.2 & 52.6 & \textbf{45.1} & 46.7 & 33.4 \\
\bottomrule
\end{tabular}
}
\endgroup
\end{table}


\subsection{Results}
Tables~\ref{tab:imagenetlt} and~\ref{tab:places365} present the results of different augmentation methods applied on top of the LIFT baseline model for the ImageNet-LT and Places365 datasets, respectively. Using SD3 as an augmentation method degrades the performance of the small classes, indicating that the samples generated by SD3 are not beneficial and instead cause the classifier to further overfit to the many and medium classes. In contrast, applying our proposed GeLDA framework achieves new SOTA performance on both datasets' small classes. GeLDA further improves the accuracy of the small classes while maintaining nearly the same performance for the many and medium classes.

In both datasets, the minimum number of samples for a class in the small category is five. To further evaluate performance under more extreme conditions, we randomly select 20 classes from the small-class subset and simulate scenarios where each of the five classes has 5-shot, 3-shot, 1-shot, and 0-shot training samples. The results are shown in Tables~\ref{tab:imagenetlt-zero-shot} and~\ref{tab:places365-zero-shot}. The proposed GeLDA method significantly improves performance on few-shot classes. This improvement becomes more pronounced as the number of shots decreases; in the 0-shot setting, our method improves accuracy by 47\% and 31\% on the ImageNet-LT and Places365 datasets, respectively. Additionally, GeLDA achieves the best overall performance across all settings. These results suggest that the proposed GeLDA framework is particularly effective in real-world scenarios where severe data imbalance is prevalent.