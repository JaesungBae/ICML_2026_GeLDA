\section{Related Workds}
\paragraph{Foundation models}
A foundation model is trained on large-scale, diverse data, typically using self-supervised or weakly-supervised learning at scale, and designed to be adaptable to a wide range of downstream tasks~\cite{on_the_opportunities_and_risks}. For example, wav2vec 2.0~\cite{wav2vec} and BERT~\cite{bert} are trained with masked prediction objectives in the speech and text domains, respectively. XLS-R~\cite{xlsr} is an expanded version of wav2vec 2.0, trained on 128 languages, to improve multilingual speech representation learning. More recently, the encoder of Whisper~\cite{whisper}, a model trained on large-scale automatic speech recognition (ASR) tasks, has proven effective as a general-purpose speech feature extractor for various non-ASR tasks such as speech emotion recognition~\cite{whisper_ser_1, emobox} and audio tagging~\cite{whisper_audio_tagging}. In the vision-language domain, CLIP~\cite{clip} has been successfully trained to learn a shared semantic space between image and text modalities. CLIP has demonstrated excellent zero-shot image classification performance by learning to associate images with descriptive text labels~\cite{online_zero_shot_classification_with_clip, clip_vqa}. Beyond classification, CLIP has also been adopted for various vision tasks such as visual question answering~\cite{clip_vqa}, image denoising~\cite{clip_image_denoising}, and face forgery detection~\cite{clip_forensic}, showcasing its strong generalization and transfer capabilities. GeLDA actively employs these foundation models to prepare the optimal feature space for the DA task. 


% \paragraph{Diffusion model with classifier-free guidance}


\paragraph{Input space data augmentation} Various transformation techniques can alter data distributions. For instance, rotation, flipping, cropping, and changes in colorization are common in vision applications~\cite{ida_image_traditional1, ida_image_traditional2} as well as mixing up examples and labels \cite{mixup, cutmix}, while adding noise~\cite{ida_importantaug}, modulating pitch~\cite{ida_cross_speaker_emotion_transfer}, or masking part of data~\cite{specaug} are common in audio. These methods tend to increase data diversity, yet they are not guaranteed to effectively recover the unseen part of the data distribution, thereby limiting the generalizability of the learned models. Recently, generative models have been widely adopted for input space DA. In speech, generative models such as voice conversion and TTS systems have increased speech variability by altering information such as speaker identity and text content. This augmentation strategy is effective across various speech-related tasks, including ASR~\cite{ida_text_is_all_you_need}, keyword spotting~\cite{ida_keyword_spotting}, TTS~\cite{ida_tts_1, ida_tts_2}, speech enhancement~\cite{ida_speech_enhancement1, ida_speech_enhamcement_2}, and emotion recognition~\cite{ida_emotion_1}. Similarly, in vision, diffusion-based image generation models have gained popularity for DA~\cite{ida_image_1, ida_image_2, ida_image_3}, as they can preserve target object information while introducing variability to attributes that are either subtle or irrelevant to the labeling task. Therefore, the key to successful DA is to generate samples associated with the desired labels, yet improve the within-class diversity.


\paragraph{Latent space data augmentation}
An intuitive way to perform DA in the latent space is to use a linear transformation or add a random perturbation to the existing latent vectors \cite{a_closer_look_at_feature_space_da, data_aug_via_latent_space, data_aug_in_feature_space, latent_filling, cheung2021modals}, assuming the feature distribution in the latent space is smooth. To further impose the latent space to be convex and simple, adversarial loss functions can be used \cite{data_aug_via_latent_space, cheung2021modals}. In \cite{feature_space_augmenation_for_lt_data, feature_space_transfer_for_da, latentaugment_da_via_guided_manipulation}, 
latent vectors are factorized and then recombined with different pairs to create new latent vectors \cite{feature_space_transfer_for_da, feature_space_augmenation_for_lt_data} or manipulated depending on the gradient \cite{latentaugment_da_via_guided_manipulation}. 
% A limitation of these methods is that they rely on a pre-trained network to correctly factorize or guide the latent vectors, which may not be reliable in low-resource scenarios where the pre-training itself could be suboptimal. 
Other works utilize consistency loss to improve generation quality for image recognition and chemical design \cite{bayseian_optimization_LDA}, and text-to-speech synthesis \cite{latent_filling}.
VAE-based methods are also common due to the simplicity of the learned latent space \cite{LeMDA, a_closer_look_at_feature_space_da}. However, generative models for DA in the latent space have been less explored compared to those operating in the input space. Furthermore, to the best of our knowledge, no prior work has investigated how the choice of latent space properties affects the effectiveness of DA. Indeed, for the first time, we propose GeLDA leveraging the powerful classifier free-guidance (CFG) for diffusion models. Moreover, we investigate how to effectively integrate such latent augmentations into general-purpose foundation models to improve performance in various data-scarcity scenarios.


% Among them, \cite{LeMDA} is the most related to our work, as it also uses a pre-trained encoder as a backbone and updates the decoder using generated latent vectors. However, their method jointly trains the generative augmentation module and the task-specific network with an additional consistency loss, which can make the training process more complicated and potentially unstable. 

% These two works augment the data in the latent space and improve the exploration region of the latent space. In \cite{latent_filling}, the authors enhance zero-shot speech synthesis by interpolating and perturbing speaker embeddings to expand the speaker representation space, and in \cite{bayseian_optimization_LDA}, the authors improve latent space Bayesian optimization by penalizing inconsistency and augmenting latent points to guide exploration toward stable and valid regions.

% It is also worth noting that most existing latent space data augmentation methods have been explored primarily in the image domain. We believe our work offers a promising step toward extending latent space data augmentation to the speech domain as well.


% Definition of foundation model: foundation model is a generalized feature extraction model that contains rich information of certain modality and can be successfully distilled to various downstream tasks. To learn the rich and general information, they trained on massive amount of datasets.

% wav2vec models are unsupervised and trained with masked prediction as BERT model. 

% Whisper~\cite{} models are trained with speech recognition task. However, much literature demonstrated that it is also possible to adopt the encoder of Whisper as a feature extractor and achieved state-of-the-art performance in various downstream tasks, such as speech emotion recognition, speech environment detection. 

% CLIP and CLAP is trained with paired data of image and text and audio and text, respectively. They adopted contrastive loss between different modalities and trained to map them in the same embedding space. They are widely adopted for image \cite{}/audio~\cite{} classification and image\cite{}/audio\cite{} generation tasks. 