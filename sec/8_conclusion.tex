\section{Conclusion and Future Work}
We introduced GeLDA, a generative latent data augmentation (DA) framework that addresses data scarcity by using diffusion models.
By operating in the low-dimensional latent space, GeLDA achieves greater efficiency than input-space DA methods. % and can be trained with small amount of data. 
We also show that conditioning GeLDA on auxiliary information about the target sub-domain helps it perform well even in low-resource and imbalanced scenarios.
% Furthermore, we show that conditioning GeLDA on various auxiliary information about the target sub-domain helps it overcome the limitations typically faced by generative models in low-resource and imbalanced scenarios.
We also revealed a trade-off in choosing the target latent space. We demonstrated its effectiveness in two practical yet challenging tasks: multi-lingual speech emotion recognition for low-resource languages and long-tailed image classification. We believe GeLDA has the potential for broader impact on real-world problems that remain underexplored due to data limitations, such as low-resource language processing, healthcare, and minority class recognition.
% , making it particularly effective in data-scarce scenarios. 
% Compared to the input space DA methods, GeLDA is more efficient and requires significantly less data, making it effective in data-scarce scenarios. We show that various auxiliary information about the target sub-domain can effectively condition GeLDA, overcoming the lack of data that also affects a generative model.

While GeLDA shows promising results, there remain several directions for further exploration. In this work, we applied it only to a lightweight task adapter with few layers. Although this ensures model-agnostic applicability, the latent spaces of the foundation model remain largely underexplored. In addition, we focused on a recognition task, leaving open the opportunity to extend GeLDA to tasks with temporal structures, such as TTS and ASR. Finally, since latent-space data is not directly perceptible, assessing its quality remains a challenge. We plan to explore more advanced evaluation methods to address this.
% While promising, GeLDA has several limitations. In this work, we applied it only to a lightweight task adapter with few layers. Although this ensures model-agnostic applicability, the latent spaces of the foundation model remain underexplored. Furthermore, we applied GeLDA only to a recognition task. Extending it to tasks with temporal structures, such as TTS and ASR, would be an interesting direction. Finally, unlike data augmentation in the input space, it is difficult to perceptually assess the quality of generated data in the latent space. We plan to explore more advanced methods for evaluating the quality of latent-space data to address this challenge.

% While promising, GeLDA has several limitations. In this work, we applied it only to a lightweight task adopter with few layers. Although this ensures model-agnostic applicability, the latent spaces in the foundation model remain underexplored. Additionally, we applied GeLDA in recognition task only. Extending it to tasks that has temporal structure such as TTS and ASR would be interesting. Finally, unlike input space DA, generated data in the latent space is hard to asses perceptually. We plan to investigate more advanced way to estimate the quality of the generated data in the latent space to address this challenge.

% limited set of task-specific latent spaces without modifying the backbone foundation model. Although this ensures model-agnostic applicability, the richer representations in the foundation model remain underexplored. Extending GeLDA to operate on these deeper latent spaces is a promising direction. Additionally, we apply GeLDA in non-temporal latent spaces. Extending it to temporal representations could enable broader applications such as text-to-speech, automatic speech recognition, and video generation or understanding. Finally, unlike input space DA, latent space DA lacks perceptual interpretability, making it difficult to assess the quality of generated samples. We plan to investigate latent-space quality estimation techniques to address this challenge.