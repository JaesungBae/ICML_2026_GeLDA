% \subsection{Multi-lingual SER for low-resource languages}

% \subsection{Experiment setups}
\section{Experiments on multi-lingual SER for low-resource languages}
\subsection{Task definition and dataset}
First, we demonstrate the effectiveness of our proposed GeLDA methods for SER in low-resource languages. In this setup, the training dataset $\mathcal{D}$ consists of the subsets $\mathcal{D}=\cup_{k=1}^K\mathcal{D}^{(k)}$ where $k$ is the language index. Each sample $x_i$ is associated with one of five emotion labels. For a low-resource language $\kappa$, we assume its samples $\mathcal{D}^{(\kappa)}$ only have the ``neutral'' emotion to simulate the real-world issue, where annotated emotional speech samples are hard to acquire.

\begin{table}[t]
    \centering
    \footnotesize
    % \vspace{-0.15in}
    \caption{Summary of multi-lingual speech emotion dataset.}
    % \vspace{-0.05in}
    \begingroup
\setlength{\tabcolsep}{3pt}
\resizebox{1.0\linewidth}{!}{  
    \begin{tabular}{lrrrrrrr|r}
        \toprule
        \textbf{Language} & \textbf{Angry} & \textbf{Disgust} & \textbf{Fear} & \textbf{Happy} & \textbf{Neutral} & \textbf{Sad} & \textbf{Surprise} & \textbf{Total} \\
        \midrule
amharic &0.21 & 0.00 & 0.29 & 0.24 & 0.27 & 0.25 & 0.00 & 1.27 \\
bangla &0.72 & 0.70 & 0.73 & 0.72 & 0.71 & 0.71 & 0.72 & 5.01 \\
english &7.89 & 4.67 & 4.41 & 9.12 & 9.45 & 8.67 & 6.27 & 50.48 \\
french &0.21 & 0.16 & 0.13 & 0.21 & 0.15 & 0.28 & 0.16 & 1.30 \\
german &0.06 & 0.03 & 0.02 & 0.03 & 0.04 & 0.04 & 0.00 & 0.22 \\
greek &0.08 & 0.08 & 0.07 & 0.08 & 0.00 & 0.10 & 0.00 & 0.41 \\
italian &0.54 & 0.64 & 0.63 & 0.51 & 0.57 & 0.65 & 0.59 & 4.13 \\
mandarin &3.06 & 0.03 & 0.07 & 2.84 & 3.42 & 3.81 & 2.51 & 15.74 \\
persian &0.62 & 0.00 & 0.02 & 0.13 & 0.84 & 0.30 & 0.06 & 1.96 \\
polish &0.03 & 0.00 & 0.02 & 0.00 & 0.02 & 0.00 & 0.00 & 0.07 \\
russian &0.23 & 0.22 & 0.24 & 0.24 & 0.18 & 0.18 & 0.00 & 1.29 \\
spanish &0.02 & 0.02 & 0.01 & 0.02 & 0.01 & 0.02 & 0.00 & 0.11 \\
turkish &0.07 & 0.00 & 0.00 & 0.06 & 0.00 & 0.08 & 0.00 & 0.21 \\
urdu &0.04 & 0.00 & 0.00 & 0.04 & 0.04 & 0.04 & 0.00 & 0.17 \\
\midrule
Total & 13.78 & 6.56 & 6.66 & 14.24 & 15.69 & 15.15 & 10.30 & 82.37 \\
        \bottomrule
    \end{tabular}
}
    \endgroup
    \label{tab:ser_dataset}
    \vspace{-0.2in}
\end{table}

We collect a multilingual speech emotion dataset from various sources, whose class and language distributions are summarized in Table \ref{tab:ser_dataset} with citations, showcasing a heavy but common imbalance issue across emotion labels and languages. 
% , which also aligns with our problem setting. 
Since we collect data from multiple sources, some emotion categories are missing. We focus on the five most common emotions, while discarding samples that belong to other than these five. All the languages except English, Chinese, and French are selected as the target low-resource languages $\kappa$. We repeat the experiments for those six target languages and report the results from six language-specific SER systems. To simulate data scarcity, when a language is selected as the low-resource target, we eliminate all its samples except those in the neutral class, repurposing $\mathcal{D}^{(\kappa)}$ into $\tilde{\mathcal{D}}^{(\kappa)}=\{(x_i, y_i)\mid y_i=\text{``neutral''}\}$. 10\% of the dataset is set as a validation and test sets, respectively, and the rest is used for training. The total number of speech samples is 67,637, and the average duration is 4 seconds.

\subsection{Implementation details and evaluation metrics}
We adopt three pretrained foundation models: XLSR \cite{xlsr} and small and large Whisper models \cite{whisper}. 
% Following the multi-task learning approach proposed in \cite{gender_ref1, gender_ref2}, we add a linear layer on top of the final model representations to predict gender information, which serves as an auxiliary task to improve emotion recognition performance. 
During the multi-lingual SER training (the first stage), the task adapter is trained from the entire training set except for the target language: $\mathcal{D}^{\setminus \kappa}=\cup_{k \neq \kappa} \mathcal{D}^{(k)}$. 
% We adopt a simple U-Net-based architecture for the diffusion model. During inference, we utilize the DDIM~\cite{ddim} with CFG++~\cite{cfgpp} for improved sampling quality.
% We set the guidance scale to $\lambda = 0.25$ and the DDIM step size to 20. Additional analyses on the effects of varying the DDIM step size and guidance scale are provided in Appendix~\ref{apdx:ddim_hyperparam}.
The diffusion model is trained with the combination $\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$, i.e., all other languages and neutral examples from the target language. It is conditioned with emotion embedding, gender embedding, and the sampled neutral utterance embedding $z^{(l)}\in\tilde{\mathcal{D}}^{(\kappa)}$, when the GeLDA is operated in $\mathcal{Z}^{(l)}$ space. 
% For conditioning, a linear layer aggregates emotion embedding, gender embedding, and the sampled neutral utterance embedding $z\in\tilde{\mathcal{D}}^{(\kappa)}$ to form the final condition vector for the CFG process. 
The trained diffusion model generates new feature vectors $\bar{z}$ for all non-neutral emotions to improve balance in $\tilde{\mathcal{D}}^{(\kappa)}$. The resulting synthetic dataset is denoted as $\bar{\mathcal{D}}^{(\kappa)}$. Finally, in the fine-tuning stage (the second training stage), the task adapter layers later than $>l$ are updated using neutral examples $\tilde{\mathcal{D}}^{(\kappa)}$, synthesized non-neutral examples $\bar{\mathcal{D}}^{(\kappa)}$, and $\mathcal{D}^{\setminus \kappa}$ that are all in the $\mathcal{Z}^{(l)}$ space. Additional implementation details can be found in Appendix~\ref{apdx:implementation_details}.

Due to the highly imbalanced nature of the emotional speech dataset, SER models are often evaluated with unweighted average (UA) recall, together with weighted average (WA) recall \cite{interspeech_emotion_challenge}. UA metric assesses per-emotion recall first and then averages them to equally account for all the emotions, preventing the domination by the most popular class. Therefore, the UA is often a better indicator of the system’s accuracy \cite{interspeech_emotion_challenge}. WA is calculated as a top-1 accuracy. 

\subsection{Comparison models} 
For the comparison models, we first build three baseline systems original data (i.e., with no augmentation): $\mathcal{D}^{\setminus \kappa}$ (missing the target language), $\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$ (including the neutral class from the target language), and $\mathcal{D}$ (the entire multi-lingual dataset for the \textit{oracle} upper-bound performance). 


As another set of baselines, we use state-of-the-art multilingual zero-shot TTS (XTTS~\cite{xtts}\footnote{\url{https://huggingface.co/coqui/XTTS-v2}}) and speech translation systems (SeamlessM4T-Large and Seamless Expressive~\cite{seamless}) to synthesize new speech signals in the input space $\mathcal{X}$.
% , bearing with the limitation in fine-tuning them with our emotional speech dataset. This is due to the missing text scripts in our dataset, which are critical in training TTS systems. 
These systems imitate speaking style and speaker identity of a provided speech prompt. For this, we randomly select English speech with the target emotion serve as prompts.
% XTTS \cite{xtts} is a zero-shot TTS system that can imitate the input speech prompt's speaking style and speaker identity, i.e., $(x_i, y_i)\sim\mathcal{D}^{\setminus \kappa}$. From \cite{seamless}, we adopt SeamlessM4T-Large and SeamlessExpressive models. SeamlessM4T-Large is a speech-to-speech translation model trained with a massive amount of multilingual dataset, and SeamlessExpressive is a version that focuses more on expressiveness by adopting expressive speech datasets. 
These models require over 20 K hours of training data and 450 M parameters, making it difficult to acquire such models for low-resource languages. In contrast, the proposed GeLDA only requires 73 hours of data and 15 M parameters, demonstrating much greater efficiency. Additional efficiency comparison is provided in Appendix~\ref{apdx:efficiency_comparison} Table \ref{tab:compare2}. % A summary of the comparison models and GeLDA’s configuration 
% , highlighting the significantly improved efficiency of our proposed GeLDA model compared to input space DA methods. 
Note that XTTS does not support Bangla, and SeamlessExpressive does not support Bangla, Korean, and Polish. Therefore, these unsupported languages are excluded when calculating the average performance for the corresponding models.
% each model.


% As another set of baselines, we use state-of-the-art multilingual zero-shot TTS \cite{xtts} and speech translation systems \cite{seamless} to synthesize new speech signals in the input space $\mathcal{X}$, bearing with the limitation in fine-tuning them with our emotional speech dataset. This is due to the missing text scripts in our dataset, which are critical in training TTS systems. 
% % We opt not to retrain open-source TTS systems as they tend not to support multiple languages, nor as good as the SOTA TTS systems regarding speech quality. 
% XTTS \cite{xtts}\footnote{\url{https://huggingface.co/coqui/XTTS-v2}} is a zero-shot TTS system that can imitate the input speech prompt's speaking style and speaker identity, i.e., $(x_i, y_i)\sim\mathcal{D}^{\setminus \kappa}$. From \cite{seamless}, we adopt SeamlessM4T-Large and SeamlessExpressive models. SeamlessM4T-Large is a speech-to-speech translation model trained with a massive amount of multilingual dataset, and SeamlessExpressive is a version that focuses more on expressiveness by adopting expressive speech datasets. 
% These models require over 20 K hours of training data and 450 M parameters, making it difficult to acquire such models for low-resource languages. In contrast, the proposed GeLDA only requires 73 hours of data and 15 M parameters, demonstrating significantly improved efficiency. A summary is provided in Appendix~\ref{apdx:efficiency_comparison} Table \ref{tab:compare2}. % of the comparison models and GeLDA’s configuration 
% % , highlighting the significantly improved efficiency of our proposed GeLDA model compared to input space DA methods. 
% Note that XTTS does not support Bangla, and SeamlessExpressive does not support Bangla, Korean, and Polish. Therefore, these unsupported languages are excluded when calculating the average performance for each model. \todo{this paragraph is TMI although I don't want to remove it completely.}

In addition to the data augmentation in $\mathcal{X}$, we also implement simple linear transformation and noise addition-based DA in $\mathcal{Z}$. Similar to the Latent Filling~\cite{latent_filling} method, we apply interpolation between the existing latent samples that share the same emotions and genders, and Gaussian noise addition. We used the same hyperparameter settings as in \cite{latent_filling}.
% the linear transformation and noise addition data augmentation methods working in $\mathcal{Z}$. Following the methods proposed in \cite{latent_filling}, 


\begin{table*}[t]
\caption{(Zero-shot Results) Average test results for multi-lingual SER models on six low-resource target languages. A star marks models evaluated only on supported languages. Bold and underlined values indicate the best and second-best performances, respectively. DA indicates the different locations where DAs are applied. Results with the Whisper-small backbone can be found in Appendix~\ref{apdx:additional-whisper-small}.}
    \label{tab:result-main}
    \centering
    \footnotesize
    \resizebox{.9\textwidth}{!}{
    \begin{tabular}{lrrrrrrr|rrrr}
%     \toprule
%     Experiment & Angry & Disgust & Fear & Happy & Neutral & Sad & Surprise & UAR & WA & Macro-F1 & UA without target emotion \\ 
%     \midrule
%     \multicolumn{12}{c}{3 Languages}\\
%     \midrule
% Baseline (pre-trained) & 65.67 & 42.33 & 38.00 & 52.00 & 98.00 & 47.67 & 23.00 & 55.83 & 57.75 & 55.36 & 47.98 \\ 
% ours baseline finetune & 54.33 & 29.00 & 31.67 & 43.33 & 99.67 & 35.67 & 16.00 & 47.88 & 49.69 & 47.31 & 38.27 \\ 
% \midrule
% seamless m4t h0 (x3) & 35.67 & 20.67 & 8.67 & 38.67 & 99.33 & 29.67 & 3.00 & 38.00 & 39.17 & 33.33 & 26.65 \\ 
% seamless expressive h0 (x3) & 51.67 & 25.33 & 36.33 & 47.33 & 97.33 & 36.33 & 23.00 & 48.23 & 49.17 & 47.80 & 39.12 \\ 
% \midrule
% ours h0 (x3) & 75.00 & 43.33 & 54.33 & 64.33 & 77.67 & 62.33 & 59.00 & 62.74 & 64.07 & 64.07 & 60.04 \\ 
% ours h1 (x3)& 65.33 & 49.67 & 51.33 & 60.00 & 46.33 & 61.67 & 43.00 & 54.98 & 55.66 & 55.92 & 56.72 \\ 
% ours h2 (x3)& 69.33 & 52.33 & 45.67 & 54.67 & 68.67 & 55.00 & 33.00 & 56.42 & 57.59 & 57.82 & 54.31 \\ 
% ours h0 (x10)& 74.00 & 41.00 & 55.67 & 65.33 & 74.67 & 63.33 & 56.00 & 62.06 & 63.47 & 62.75 & 59.70 \\ 
% ours h1 (x10)& 61.33 & 52.00 & 43.00 & 58.00 & 46.67 & 55.00 & 46.00 & 52.27 & 52.30 & 53.59 & 53.34 \\ 
% ours h2 (x10)& 64.33 & 56.00 & 40.33 & 54.33 & 65.67 & 59.67 & 34.00 & 55.61 & 56.25 & 57.07 & 53.87 \\ 
% \midrule
% \multicolumn{12}{c}{10 Languages}\\
% \midrule
% Baseline (pre-trained) & 60.50 & 31.17 & 34.22 & 44.56 & 96.20 & 36.22 & 30.75 & 51.05 & 52.54 & 48.16 & 40.25 \\ 
% ours baseline finetune & 47.30 & 22.83 & 28.11 & 33.89 & 99.70 & 26.33 & 25.00 & 43.83 & 45.57 & 40.50 & 30.49 \\ 
% \midrule
% ours h0 (x3)& 70.50 & 34.00 & 48.22 & 59.33 & 73.60 & 40.22 & 59.25 & 54.84 & 55.91 & 54.87 & 51.02 \\ 
% ours h1 (x3)& 67.80 & 38.00 & 44.33 & 58.33 & 45.40 & 45.67 & 55.75 & 49.54 & 49.45 & 49.35 & 50.57 \\ 
% ours h2 (x3)& 70.30 & 38.33 & 41.78 & 54.56 & 62.40 & 46.67 & 51.00 & 52.44 & 53.09 & 52.70 & 50.60 \\ 
% ours h0 (x10)& 69.20 & 29.33 & 48.00 & 59.11 & 73.40 & 41.11 & 60.50 & 54.20 & 55.08 & 53.80 & 50.22 \\ 
% ours h1 (x10)& 66.40 & 39.33 & 40.67 & 55.67 & 42.20 & 42.89 & 55.25 & 47.65 & 47.44 & 48.29 & 48.99 \\ 
% ours h2 (x10)& 68.40 & 40.50 & 39.78 & 55.11 & 59.90 & 47.22 & 52.00 & 51.34 & 51.91 & 51.78 & 49.73 \\ 
    \toprule
    Experiment & Angry & Disgust & Fear & Happy & Neutral & Sad & Surprise & UAR & WA & Macro-F1 & UA without target emotion \\ 
    \midrule
    \multicolumn{12}{c}{Whisper}\\
    \midrule
Baseline (pre-trained) & 76.33 & 44.33 & 42.33 & 51.67 & 94.67 & 56.33 & 63.00 & 60.97 & 62.61 & 60.56 & 54.47 \\
seamless expressive h0 & 62.67 & 37.67 & 37.00 & 50.00 & 91.67 & 49.67 & 62.00 & 55.05 & 56.04 & 55.33 & 48.03 \\
seamless m4t h0 & 54.33 & 32.67 & 13.33 & 52.67 & 97.00 & 49.67 & 46.00 & 49.74 & 50.63 & 46.91 & 40.78 \\
ours baseline finetune baseline finetune & 67.33 & 35.67 & 36.67 & 46.00 & 99.00 & 46.33 & 51.00 & 54.97 & 56.21 & 54.61 & 46.64 \\
ours fixed h0 & 79.67 & 53.67 & 59.33 & 64.67 & 75.67 & 56.33 & 64.00 & 65.05 & 66.26 & 65.17 & 63.13 \\
ours fixed h1 & 74.00 & 61.33 & 47.00 & 61.00 & 62.33 & 61.33 & 64.00 & 61.33 & 61.83 & 61.01 & 61.29 \\
ours fixed h2 & 78.67 & 53.67 & 52.00 & 62.33 & 69.00 & 60.00 & 68.00 & 62.86 & 63.99 & 62.62 & 61.79 \\
\midrule
    \multicolumn{12}{c}{WavLM-Large}\\
    \midrule
Baseline (pre-trained) & 72.33 & 45.33 & 32.33 & 44.00 & 90.67 & 48.67 & 45.00 & 55.05 & 55.44 & 54.06 & 48.21 \\ 
seamless expressive h0 & 55.33 & 33.00 & 27.67 & 32.00 & 92.00 & 53.67 & 42.00 & 48.63 & 48.59 & 46.52 & 40.34 \\ 
seamless m4t h0 & 44.33 & 25.00 & 26.67 & 34.33 & 91.33 & 44.00 & 42.00 & 44.20 & 44.42 & 42.32 & 35.26 \\ 
ours baseline finetune baseline finetune & 56.67 & 28.00 & 22.33 & 33.00 & 99.67 & 38.67 & 23.00 & 45.27 & 45.60 & 42.65 & 34.99 \\ 
ours fixed h0 & 73.00 & 46.33 & 38.33 & 44.33 & 58.67 & 60.67 & 45.00 & 53.12 & 53.73 & 52.56 & 52.27 \\ 
ours fixed h1 & 70.00 & 46.00 & 48.33 & 43.00 & 52.00 & 52.00 & 48.00 & 51.61 & 52.82 & 50.74 & 51.72 \\ 
ours fixed h2 & 73.33 & 47.67 & 34.33 & 54.33 & 55.67 & 62.67 & 52.00 & 54.40 & 54.81 & 53.54 & 54.36 \\ 
\midrule
    \multicolumn{12}{c}{emotion2vec-base}\\
    \midrule
    
Baseline (pre-trained) & 56.33 & 35.33 & 27.00 & 27.67 & 88.67 & 50.67 & 32.00 & 46.54 & 46.21 & 44.02 & 38.43 \\ 
seamless expressive h0 & 43.67 & 23.33 & 26.67 & 18.33 & 92.00 & 50.33 & 22.00 & 41.37 & 41.37 & 38.71 & 31.67 \\ 
seamless m4t h0 & 20.67 & 21.33 & 13.67 & 9.67 & 96.67 & 25.67 & 16.00 & 30.51 & 29.10 & 25.63 & 17.99 \\ 
ours baseline finetune baseline finetune & 14.00 & 8.33 & 10.67 & 6.00 & 99.67 & 9.67 & 14.00 & 24.17 & 23.51 & 17.36 & 9.87 \\ 
ours fixed h0 & 50.33 & 44.67 & 38.00 & 29.00 & 76.33 & 47.33 & 32.00 & 46.85 & 46.25 & 45.35 & 41.32 \\ 
ours fixed h1 & 51.00 & 38.33 & 36.67 & 30.00 & 51.00 & 51.33 & 35.00 & 42.46 & 42.02 & 41.14 & 40.97 \\ 
ours fixed h2 & 61.00 & 46.00 & 32.67 & 34.33 & 54.00 & 54.00 & 36.00 & 46.26 & 46.03 & 45.56 & 44.89 \\ 


\bottomrule
    \end{tabular}
}
\end{table*}
\todo[inline]{Table~\ref{tab:result-main} (10-language setting) might move to the appendix.}

% \begin{table*}[t]
% \caption{Average test results for multi-lingual SER models on six low-resource target languages. A star marks models evaluated only on supported languages. Bold and underlined values indicate the best and second-best performances, respectively. DA indicates the different locations where DAs are applied. Results with the Whisper-small backbone can be found in Appendix~\ref{apdx:additional-whisper-small}.}
%     \label{tab:result-main}
%     \centering
%     \footnotesize
%     \resizebox{.9\textwidth}{!}{
%     \begin{tabular}{lrrrrrrr|rrr}
%     \toprule
%     Experiment & Angry & Disgust & Fear & Happy & Neutral & Sad & Surprise & UAR & WA & Macro-F1 \\ 
%     \midrule
%     \multicolumn{11}{c}{3 Languages}\\
%     \midrule
% Baseline (pre-trained) & 86.00 & 73.00 & 69.00 & 76.00 & 91.00 & 76.00 & 71.00 & 77.99 & 78.48 & 78.31 \\ 
% ours baseline finetune & 87.27 & 75.98 & 68.93 & 77.69 & 91.43 & 78.34 & 71.67 & 79.67 & 79.96 & 79.82 \\ 
% \midrule
% seamless m4t h0 (x3) & 82.78 & 74.11 & 64.22 & 79.00 & 88.33 & 76.67 & 73.33 & 77.44 & 78.04 & 77.51 \\ 
% + Bootstrapping & 87.89 & 80.33 & 72.89 & 80.78 & 90.11 & 77.56 & 68.00 & 81.15 & 81.18 & 80.93 \\ 
% seamless expressive h0 (x3) & 83.89 & 70.11 & 67.00 & 77.67 & 89.89 & 76.00 & 70.33 & 77.21 & 78.10 & 77.09 \\ 
% + Bootstrapping & 84.22 & 72.11 & 74.22 & 80.78 & 89.00 & 76.44 & 71.67 & 79.29 & 79.87 & 79.50 \\ 
% \midrule
% Latent Filling h0 & 88.44 & 78.11 & 72.33 & 83.22 & 92.11 & 81.56 & 73.33 & 82.37 & 82.67 & 82.55 \\ 
% Latent Filling h1 & 88.78 & 74.22 & 74.44 & 82.00 & 90.89 & 78.67 & 70.67 & 81.16 & 81.52 & 81.37 \\ 
% Latent Filling h2 & 88.22 & 73.11 & 71.11 & 77.78 & 92.00 & 79.56 & 73.00 & 80.13 & 80.57 & 80.33 \\ 
% \midrule
% ours h0 (x3) & 85.84 & 73.19 & 74.52 & 83.31 & 93.19 & 77.05 & 72.56 & 80.89 & 81.36 & 80.90 \\ 
% + Bootstrapping & 87.88 & 76.15 & 74.93 & 83.94 & 92.17 & 84.68 & 70.64 & 82.90 & 83.06 & 82.89 \\ 
% ours h1 (x3) & 84.79 & 75.12 & 70.24 & 79.58 & 88.48 & 75.27 & 67.31 & 78.52 & 79.00 & 78.67 \\ 
% + Bootstrapping & 87.31 & 76.74 & 73.32 & 78.67 & 90.50 & 78.99 & 73.97 & 80.76 & 81.13 & 80.88 \\ 
% ours h2 (x3) & 85.67 & 73.96 & 68.61 & 76.33 & 89.27 & 77.31 & 68.33 & 78.20 & 78.60 & 78.21 \\ 
% + Bootstrapping & 86.67 & 74.52 & 69.76 & 78.15 & 89.95 & 78.17 & 69.49 & 79.20 & 79.60 & 79.26 \\ 
% \midrule
% ours h0 (x10) & 87.39 & 77.94 & 75.72 & 85.25 & 93.15 & 79.21 & 71.67 & 82.77 & 83.07 & 82.78 \\ 
% + Bootstrapping  & 87.08 & 79.70 & 78.81 & 83.68 & 92.23 & 81.30 & 67.44 & 83.24 & 83.45 & 83.24 \\ 
% ours h1 (x10) & 83.92 & 74.51 & 70.27 & 81.85 & 88.78 & 77.08 & 69.62 & 79.15 & 79.50 & 79.23 \\ 
% + Bootstrapping  & 83.73 & 76.73 & 73.54 & 84.76 & 92.89 & 79.39 & 71.67 & 81.56 & 81.75 & 81.61 \\ 
% ours h2 (x10) & 86.37 & 74.93 & 69.55 & 79.55 & 88.08 & 77.56 & 69.62 & 79.05 & 79.36 & 79.03 \\ 
% + Bootstrapping & 86.31 & 72.85 & 69.28 & 76.98 & 90.10 & 78.44 & 70.26 & 78.72 & 79.15 & 78.73 \\ 

% \midrule
% \multicolumn{11}{c}{12 Languages}\\
% \midrule
% Baseline (pre-trained) & 88.00 & 66.00 & 68.00 & 79.00 & 81.00 & 75.00 & 72.00 & 76.93 & 77.84 & 76.46 \\ 
% ours baseline finetune & 88.30 & 69.03 & 66.96 & 79.12 & 80.87 & 76.22 & 71.84 & 77.29 & 78.29 & 76.75 \\ 
% \midrule
% Latent Filling h0 & 89.64 & 71.67 & 66.10 & 82.55 & 83.17 & 77.73 & 75.75 & 79.39 & 80.57 & 78.77 \\ 
% Latent Filling h1 & 89.11 & 70.00 & 70.50 & 81.27 & 81.27 & 76.27 & 75.25 & 78.67 & 79.65 & 78.10 \\ 
% Latent Filling h2 & 89.03 & 67.62 & 69.93 & 80.30 & 81.20 & 77.76 & 74.33 & 78.35 & 79.25 & 77.73 \\ 
% \midrule
% ours h0 (x3) & 88.58 & 68.75 & 73.32 & 82.92 & 84.36 & 79.06 & 71.92 & 80.02 & 80.39 & 78.92 \\ 
% + Bootstrapping  & 90.40 & 70.75 & 73.95 & 84.07 & 82.62 & 80.78 & 70.62 & 80.88 & 81.16 & 79.58 \\ 
% ours h1 (x3) & 85.53 & 69.56 & 72.48 & 80.63 & 80.21 & 75.98 & 73.10 & 78.00 & 78.21 & 76.73 \\ 
% + Bootstrapping & 87.00 & 70.25 & 72.78 & 80.50 & 80.87 & 77.87 & 75.90 & 78.78 & 79.10 & 77.68 \\ 
% ours h2 (x3)  & 87.82 & 67.57 & 68.28 & 78.68 & 80.42 & 75.07 & 72.49 & 77.15 & 77.91 & 76.25 \\ 
% + Bootstrapping & 88.43 & 68.14 & 68.60 & 79.52 & 80.89 & 76.25 & 73.17 & 77.70 & 78.47 & 76.84 \\ 
% \midrule
% ours h0 (x10) & 88.35 & 71.54 & 73.75 & 84.60 & 83.49 & 79.62 & 71.80 & 80.65 & 81.01 & 79.52 \\ 
% + Bootstrapping & 87.98 & 73.68 & 73.39 & 84.83 & 83.32 & 81.09 & 70.32 & 81.01 & 81.42 & 79.77 \\ 
% ours h1 (x10) & 85.98 & 68.50 & 71.79 & 82.56 & 81.64 & 78.17 & 73.18 & 78.86 & 79.13 & 77.62 \\ 
% + Bootstrapping & 87.64 & 71.10 & 73.38 & 83.74 & 83.31 & 79.08 & 75.21 & 80.20 & 80.38 & 78.89 \\ 
% ours h2 (x10) & 87.70 & 66.49 & 67.91 & 79.34 & 80.68 & 76.36 & 71.90 & 77.35 & 78.04 & 76.31 \\ 
% + Bootstrapping & 87.75 & 67.29 & 68.50 & 79.92 & 80.90 & 77.04 & 73.26 & 77.70 & 78.39 & 76.77 \\ 
% \bottomrule
%     \end{tabular}
%     }
% \end{table*}

\iffalse
\begin{table}[t]
    \caption{Average test results for multi-lingual SER models on six low-resource target languages. A star marks models evaluated only on supported languages. Bold and underlined values indicate the best and second-best performances, respectively. DA indicates the different locations where DAs are applied. Results with the Whisper-small backbone can be found in Appendix~\ref{apdx:additional-whisper-small}.}
    \label{tab:result-main}
    \centering
    \footnotesize
    \resizebox{.9\textwidth}{!}{
    \begin{tabular}{lccccccrr}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{DA}} & \multicolumn{5}{c}{\textbf{Per emotion recall (\%)}} & 
        \multirow{2}{*}{\textbf{UA (\%)}} & \multirow{2}{*}{\textbf{WA (\%)}} \\ 
        \cline{3-7}
        ~ & ~ &\textbf{Angry} & \textbf{Happy} & \textbf{Neutral} & \textbf{Sad} & \multicolumn{1}{c}{\textbf{Surprise}} & ~ & ~\\
        \midrule
        \rowcolor{gray!20}\multicolumn{9}{c}{Backbone foundation model: XLSR} \\
        Baseline ($\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$) & - & 41.97 & 41.80 & 98.00 & 35.00 & 20.10 & 47.37 & 65.07 \\
        Baseline ($\mathcal{D}^{\setminus \kappa}$) & - & 61.79 & 51.78 & 60.60 & 58.06 & 24.57 & 51.36 & 59.27\\ 
        \hline
        XTTS* & ~ &47.48 & 44.64 & 87.78 & 35.54 & 24.52 & 47.99 
 & \textbf{64.89}\\ 
        Seamless M4T v2 & $\mathcal{X}$ & 45.77 & 39.60 & 90.29 & 37.99 & 20.18 & 46.76 & 62.13\\ 
        Seamless Expressive* & ~ & 46.98 & 50.90 & 82.55 & 47.55 & 17.56 & 49.11 & 60.62\\
        \hline
        ~ & $\mathcal{Z}^{(0)}$ & 54.24 & 56.55 & 75.77 & 52.6 & 23.51 & 52.54 & 63.44 \\ 
        Latent Filling & $\mathcal{Z}^{(1)}$ &55.48 & 58.55 & 61.49 & 53.36 & 26.08 & 50.99 & 59.91\\ 
        ~ & $\mathcal{Z}^{(2)}$ & 59.31 & 57.43 & 60.17 & 55.48 & 24.57 & 51.39 & 59.64 \\ 
        \hline
        ~ & $\mathcal{Z}^{(0)}$ & 51.37 & 44.81 & 90.98 & 41.07 & 23.68 & 50.38 & \underline{64.69}\\ 
        GeLDA (ours) & $\mathcal{Z}^{(1)}$ & 60.71 & 58.75 & 56.00 & 57.22 & 35.83 & \textbf{53.70} & 59.62\\ 
        ~ & $\mathcal{Z}^{(2)}$ & 60.51 & 56.95 & 61.22 & 57.87 & 28.33 & \underline{52.97} & 59.93\\ 
        % ~ & Proposed - DDIM & 44.79 & 39.42 & 96.94 & 50.52 & 57.27 & 57.79 & 57.62 \\ 
        % ~ & Proposed - DDPM & 46.88 & 46.15 & 97.96 & 44.33 & 52.73 & 57.61 & 57.43 \\ 
        % ~ & Proposed - DDIM CFG++ & 48.96 & 50.96 & 92.86 & 50.52 & 62.73 & 61.20 & 61.19 \\ 
        % ~ & Proposed - DDIM CFG++ (x5) & 50.00 & 52.88 & 92.86 & 53.61 & 70.00 & 63.87 & 63.96 \\ 
        \hline
        Baseline ($\mathcal{D}$)& - & 75.52 & 59.97 & 79.20 & 68.83 & 38.66 & 64.43 & 74.19\\ 
        % ~ & + Fine-tuned (All) &  \\ 
        
        \midrule
        % \rowcolor{gray!20}\multicolumn{9}{c}{Backbone foundation model: Whisper-small} \\
        % Baseline ($\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$) & - & 35.70 & 32.32 & 99.71 & 23.65 & 16.06 & 41.48 & 60.85 \\
        % Baseline ($\mathcal{D}^{\setminus \kappa}$)  & - &61.26 & 59.77 & 65.54 & 58.29 & 27.48 & 54.47 & 63.35\\
        % \hline
        % XTTS* & ~ & 41.57 & 35.15 & 95.36 & 31.76 & 17.81 & 44.33 & \underline{65.89}\\
        % Seamless M4T v2 & $\mathcal{X}$ & 42.12 & 34.23 & 94.50 & 28.70 & 16.23 & 43.16 & 60.98\\
        % Seamless Expressive* & ~ & 59.03 & 45.91 & 83.45 & 51.67 & 28.89 & 53.79 & 64.57\\
        % \hline
        % ~ & $\mathcal{Z}^{(0)}$ & 45.31 & 45.16 & 94.85 & 37.95 & 19.78 & 48.61 & 64.45\\
        % Latent Filling & $\mathcal{Z}^{(1)}$ & 55.39 & 56.7 & 82.65 & 53.16 & 24.84 & 54.55 & \textbf{67.09}\\
        % ~ & $\mathcal{Z}^{(2)}$ & 60.57 & 59.15 & 69.44 & 57.41 & 27.24 & 54.76 & 63.89\\
        % \hline
        % ~ & $\mathcal{Z}^{(0)}$ & 44.79 & 34.34 & 95.17 & 29.70 & 21.64 & 45.12 & 63.60\\
        % Proposed &$\mathcal{Z}^{(1)}$& 58.02 & 44.54 & 79.64 & 59.07 & 33.32 & \underline{54.92} & 65.84\\
        % ~ & $\mathcal{Z}^{(2)}$& 60.92 & 57.95 & 69.49 & 61.38 & 35.74 & \textbf{57.09} & 64.24\\
        % \hline
        % Baseline ($\mathcal{D}$)& - & 77.22 & 68.43 & 86.06 & 69.38 & 36.28 & 67.47 & 78.91\\
        % \midrule

        \rowcolor{gray!20}\multicolumn{9}{c}{Backbone foundation model: Whisper-large} \\
        Baseline ($\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$) & - & 45.01 & 39.61 & 98.97 & 36.25 & 24.81 & 48.93 & 66.82\\
        Baseline ($\mathcal{D}^{\setminus \kappa}$)  & - &70.39 & 67.98 & 68.28 & 74.08 & 34.16 & 62.98 & 68.96\\
        \hline
        XTTS* & ~ & 41.87 & 43.88 & 93.94 & 41.19 & 26.95 & 49.57 & 68.91 \\
        Seamless M4T v2 & $\mathcal{X}$ & 51.02 & 49.61 & 94.36 & 40.40 & 29.04 & 52.89 & 69.60 \\
        Seamless Expressive* & ~ & 57.05 & 57.32 & 88.53 & 51.33 & 32.79 & 57.40 & 68.60 \\
        \hline
        ~ & $\mathcal{Z}^{(0)}$& 59.39 & 58.23 & 93.13 & 56.85 & 33.11 & 60.14 & \textbf{73.56} \\
        Latent Filling & $\mathcal{Z}^{(1)}$ & 69.06 & 68.03 & 78.00 & 70.92 & 34.05 & 64.01 & \underline{73.35} \\
        ~ & $\mathcal{Z}^{(2)}$& 70.14 & 68.64 & 69.69 & 73.73 & 34.35 & 63.31 & 69.57\\
        \hline
        ~ & $\mathcal{Z}^{(0)}$ & 54.91 & 51.25 & 96.13 & 41.79 & 28.75 & 54.57 & 71.18\\
        GeLDA (ours) & $\mathcal{Z}^{(1)}$ & 72.12 & 63.07 & 79.17 & 70.01 & 43.09 & \textbf{65.49} & 72.36 \\
        ~ & $\mathcal{Z}^{(2)}$  & 69.96 & 65.85 & 70.23 & 72.93 & 41.56 & \underline{64.11} & 69.05\\
        \hline
        ~ & $\mathcal{Z}^{(0)}$ & 72.21 & 66.33 & 84.59 & 64.82 & 36.56 & 64.90 & \textbf{73.79}\\
        GeLDA $\times 20$ (ours) & $\mathcal{Z}^{(1)}$  & 71.78 & 68.16 & 70.12 & 74.62 & 48.72 & \textbf{66.68} & 70.78\\
        ~ & $\mathcal{Z}^{(2)}$ & 69.97 & 67.43 & 69.61 & 71.86 & 42.95 & 64.36 & 69.16\\
        
        % ~ & Proposed - DDIM CFG++  & 41.67 & 56.73 & 92.86 & 65.98 & 68.18 & 65.08 & 65.15 \\
        \hline
        Baseline ($\mathcal{D}$) & - & 83.53 & 74.22 & 84.98 & 76.54 & 44.72 & 72.80 & 81.41\\
        % ~ & + Finetuned (All) & 84.38 & 76.50 & 80.80 & 79.94 & 56.57 & 75.64 & 79.66\\
        
        \bottomrule
    \end{tabular}
    }
\end{table}
\fi


\subsection{Results}
Table~\ref{tab:result-main} presents the average results across six target languages. Results are divided into two parts based on the two foundation models used: XLSR and Whisper-large. 
% Since UA better reflects performance across all emotions~\cite{}, we focus on UA in our analysis.
% The average results of multi-lingual SER models with six target languages are in Table~\ref {tab:result-main}. Please note that we are focusing more on UA, since it is well representing metrics for overall emotional performance. 
The baselines trained with $\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$ perform the worst in terms of UA due to the bias toward neutral emotion in the target language $\tilde{\mathcal{D}}^{(\kappa)}$. 
% Similarly, if a data augmentation method shows high accuracy on neutral emotion but poor accuracy on other emotions, it indicates that the augmented data fails to represent non-neutral emotions effectively. 
$\mathcal{D}^{\setminus \kappa}$ represents a zero-shot condition, i.e., language $\kappa$ is missing entirely.  
% The baseline model trained with $\mathcal{S}_\ell$ performs the worst for all foundation models, and this is because when the systems are only trained with neutral emotion for target languages, the results are highly biased to neutral emotion. Similarly, in the evaluation for the data augmentation methods, if the neutral emotion's accuracy is relatively higher than other emotions, we can interpret it as the augmented data does not effectively represent the other emotions correctly, and the systems are biased to neutral emotions during fine-tuning. The baseline model with $\mathcal{S}$ dataset can be interpreted as a zero-shot performance of the SER systems, since they never see the target language's speech samples during training. 

Applying input space DA does not bring performance improvement. We attribute this to the limited emotional expressiveness of synthetic speech. Among these, SeamlessExpressive achieves the best results, highlighting the importance of expressive generation. However, all TTS models fail to improve the class imbalance (e.g., poor accuracy in ``surprise'').
% When we applied input space data augmentation methods, the performance deteriorated for all the foundation models. We believe that this is because of the artificial noises introduced by generative models, and they are unable to generate speech's emotion properly. Among the three of them, Seamless Expressive models perform the best, highlighting the importance of the expressiveness of the generated speech. We believe that using a speech generation model with better expresses the emotional speech will improve the performance, but it is worth noting that for low-resource languages, it is problematic to acquire it. 
Simple latent DA, Latent Filling, also fails to improve upon the zero-shot baseline ($\mathcal{D}^{\setminus \kappa}$), although it outperforms TTS-based DA, implying a well-defined latent space is effective for DA.
% For the latent space data augmentation methods with simple statistical-based methods, the performance is on par with zero-shot baseline systems ($\mathcal{S}$) for XLSR and Whisper-small backbone foundation models, while it improves the performance by 1\% for the Whisper-large model. For the statistical-based latent filling to be effective, a well-learned latent space is necessary.

With our proposed GeLDA method, UA improves by 2.34\% and 2.51\% points for XLS and Whisper-large, respectively. Given the upper bound improvement by using oracle data $\mathcal{D}$ is 9.82\% for the Whisper-large backbone model, these improvements are substantial. Among different latent spaces, augmentation at $\mathcal{Z}^{(1)}$ consistently outperforms others, revealing the tradeoff in choosing the right latent space.
Figure~\ref{fig:result-embedding-bangla} shows t-SNE plots of ground-truth (GT) test embeddings and generated vectors in Bangla. Augmentation at $\mathcal{Z}^{(0)}$ produces samples far from the GT distribution, likely due to the irrelevant information in the low-level feature space. However, augmented data at $\mathcal{Z}^{(1)}$ and $\mathcal{Z}^{(2)}$ show better harmonization with the GT embeddings.
Comparing $\mathcal{Z}^{(1)}$ and $\mathcal{Z}^{(2)}$, although GeLDA appears to be more successful in $\mathcal{Z}^{(2)}$, it limits the following fine-tuning process's capability.   
% However, as shown in Figure~\ref{fig:result-z-bangla}, when both are projected into the same $\mathcal{Z}^{(2)}$ space after fine-tuning, the augmentation in the $\mathcal{Z}^{(1)}$ space exhibit better clustering by emotion.


% However, when it is compared them in the same $\mathcal{Z}^{(2)}$ space in Figure~\ref{fig:result-z-bangla} after fine-tuning, $\mathcal{Z}^{(1)}$ case is more clusterable depend on the emotions.


% Further, Figure~\ref{fig:result-z-bangla} shows the effect of fine-tuning on $\mathcal{Z}^{(2)}$ space. While augmentation at $\mathcal{Z}^{(2)}$ appears well-aligned before fine-tuning, it cannot influence earlier layers. In contrast, augmenting at $\mathcal{Z}^{(1)}$ enables earlier layers to adapt, resulting in more clustered emotion representations after fine-tuning.
% 
% With our proposed generative latent space data augmentation, the UA improves 2.35\%, 2.62\%, and 2.51\% for XLSR, Whisper-small, and Whisper-large backbone models, respectively. Especially considering that the gap between the upper bound and the baseline model is 9.82\%, our proposed method's improvement is significant. Comparing different latent space locations, $\mathcal{Z}^{(1)}$ performs the best for the XLSR and  Whisper-large foundation model. In Figure~\ref{fig:result-embedding-bangla}, the t-SNE plot of the test ground-truth embedding and generated latent vector  Whisper-large model backbone for the Bangla language can be found. We can observe that at $\mathcal{Z}^{(0)}$ space, the generated data and GT data have different properties and can be separable. We believe that this is because the $\mathcal{Z}^{(0)}$ still contains rich information that is not relevant to the target task, and might be a burden to synthesize them with our simple diffusion-based generative models. However, the generated embeddings are well harmonized with GT embeddings in $\mathcal{Z}^{(1)}$ and $\mathcal{Z}^{(2)}$. 

% the layer before the $\mathcal{Z}^{(2)}$ space, but when we augment in $\mathcal{Z}^{(2)}$, we cannot update any layers before that.

In summary, the proposed GeLDA method can significantly improve the language-specific SER results by using augmented data with a matching language label.  More detailed results on each language can be found in Appendix \ref{apdx:each_language}. Additional SER results with Whisper-small, visualization of the augmented features before and after fine-tuning, and language-agnostic GeLDA for multilingual SER are also presented in \ref{app:additional_ser}.

% $\mathcal{Z}^{(2)}$ looks more separable than $\mathcal{Z}^{(1)}$ in Figure~\ref{fig:result-aug}. This is because $\mathcal{Z}^{(2)}$ space is a space right before the output space, therefore, the embeddings space should be linearly separable


\iffalse
\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.28\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/uar_iter.png}
  \end{minipage}
  % \hspace{0.03\linewidth}
  % \begin{minipage}[b]{0.24\linewidth}
  %   \centering
  %   \includegraphics[width=\linewidth]{figs/WA_iter.png}
  % \end{minipage}
  % \hspace{0.03\linewidth}
  \begin{minipage}[b]{0.28\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/neutral_iter.png}
  \end{minipage}
  % \hspace{0.03\linewidth}
  \begin{minipage}[b]{0.40\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/others_iter.png}
  \end{minipage}
  \caption{Trend graph as the number of fine-tuning iterations increases with different augmented latent vector locations. (Left) UA, (middle) accuracy of neutral emotion, (right) average accuracy of non-neutral emotions. }
  \label{fig:result-iter}
\end{figure}
\fi

% \vspace{-10pt}



  % (Left) UA, (middle) accuracy on neutral emotion, and (right) average accuracy on non-neutral emotions.


\begin{figure}[ht]
  \centering
  % \vspace{-0.35in}
  \includegraphics[width=\linewidth]{fig/graph_whisper-large-v3_combined_3langs.pdf}
  % \vspace{-0.07in}
  \caption{SER model architecture with an example of performing GeLDA in the $\mathcal{Z}^{(1)}$ space. $\mathcal{S}(\cdot)$ indicates the diffusion model trained to generate latent vectors, while $u(\kappa)$ represents the conditioning information about the target language $\kappa$.}
  \label{fig:model}
\end{figure}


\paragraph{Effect of additional data augmentation}
One of the biggest strengths of data augmentation is that we can generate an unlimited amount of data. To explore the effectiveness of additional data augmentation, we generate latent vectors 20 times more. The results of the case of Whisper-large backbone are described in Table~\ref{tab:result-main} as ``GeLDA $\times$ 20''. The additional data introduces an additional 1.19\% point, although its efficacy saturates around that point. 
% % , and the trend graphs can be found in Figure~\ref{fig:result-aug}. For $\mathcal{Z}^{(1)}$ space, the UA improves additional 1.19\%, while for $\mathcal{Z}^{(2)}$ space, the performance doesn't improve significantly, but for $\mathcal{Z}^{(0)}$ space, the UA improves by 10.33\%. \todo{why?}

% \begin{wrapfigure}{r}{0.35\textwidth}
%   \centering
%   \vspace{-0.2in}
%   \includegraphics[width=1.0\linewidth]{figs2/uar_iter_x10.pdf}
%   \caption{UA trends of the Whisper-large backbone model as the number of augmented samples increases.}
%   \vspace{-25pt}
%   \label{fig:result-aug}
% \end{wrapfigure}
    

