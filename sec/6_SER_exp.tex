% \subsection{Multi-lingual SER for low-resource languages}

% \subsection{Experiment setups}
\section{Experiments on Zero-shot Speech Emotion Recognition}
\subsection{Task definition and dataset}
First, we demonstrate the effectiveness of our proposed GeLDA methods for SER in low-resource languages. In this setup, the training dataset $\mathcal{D}$ consists of the subsets $\mathcal{D}=\cup_{k=1}^K\mathcal{D}^{(k)}$ where $k$ is the language index. Each sample $x_i$ is associated with one of five emotion labels. For a low-resource language $\kappa$, we assume its samples $\mathcal{D}^{(\kappa)}$ only have the \textit{``neutral''} emotion to simulate the real-world issue, where annotated emotional speech samples are hard to acquire.

\begin{table}[t]
    \centering
    \footnotesize
    % \vspace{-0.15in}
    \caption{Summary of multi-lingual speech emotion dataset in hours.}
    % \vspace{-0.05in}
    \begingroup
\setlength{\tabcolsep}{3pt}
\resizebox{1.0\linewidth}{!}{  
    \begin{tabular}{lrrrrrrr|r}
        \toprule
        \textbf{Language} & \textbf{Angry} & \textbf{Disgust} & \textbf{Fear} & \textbf{Happy} & \textbf{Neutral} & \textbf{Sad} & \textbf{Surprise} & \textbf{Total} \\
        \midrule
amharic &0.21 & 0.00 & 0.29 & 0.24 & 0.27 & 0.25 & 0.00 & 1.27 \\
bangla &0.72 & 0.70 & 0.73 & 0.72 & 0.71 & 0.71 & 0.72 & 5.01 \\
english &7.89 & 4.67 & 4.41 & 9.12 & 9.45 & 8.67 & 6.27 & 50.48 \\
french &0.21 & 0.16 & 0.13 & 0.21 & 0.15 & 0.28 & 0.16 & 1.30 \\
german &0.06 & 0.03 & 0.02 & 0.03 & 0.04 & 0.04 & 0.00 & 0.22 \\
greek &0.08 & 0.08 & 0.07 & 0.08 & 0.00 & 0.10 & 0.00 & 0.41 \\
italian &0.54 & 0.64 & 0.63 & 0.51 & 0.57 & 0.65 & 0.59 & 4.13 \\
mandarin &3.06 & 0.03 & 0.07 & 2.84 & 3.42 & 3.81 & 2.51 & 15.74 \\
persian &0.62 & 0.00 & 0.02 & 0.13 & 0.84 & 0.30 & 0.06 & 1.96 \\
polish &0.03 & 0.00 & 0.02 & 0.00 & 0.02 & 0.00 & 0.00 & 0.07 \\
russian &0.23 & 0.22 & 0.24 & 0.24 & 0.18 & 0.18 & 0.00 & 1.29 \\
spanish &0.02 & 0.02 & 0.01 & 0.02 & 0.01 & 0.02 & 0.00 & 0.11 \\
turkish &0.07 & 0.00 & 0.00 & 0.06 & 0.00 & 0.08 & 0.00 & 0.21 \\
urdu &0.04 & 0.00 & 0.00 & 0.04 & 0.04 & 0.04 & 0.00 & 0.17 \\
\midrule
Total & 13.78 & 6.56 & 6.66 & 14.24 & 15.69 & 15.15 & 10.30 & 82.37 \\
        \bottomrule
    \end{tabular}
}
    \endgroup
    \label{tab:ser_dataset}
    \vspace{-0.2in}
\end{table}

We collect a multilingual speech emotion dataset following EmoBox~\cite{emobox}, and its language distribution is summarized in Table \ref{tab:ser_dataset} with citations, showcasing a heavy but common imbalance issue across emotion labels and languages. 
% , which also aligns with our problem setting. 
% Since we collect data from multiple sources, some emotion categories are missing. We focus on the five most common emotions, while discarding samples that belong to other than these five. 
% All the languages except English and Chinese are selected as the target low-resource languages $\kappa$. We repeat the experiments for those six target languages and report the results from six language-specific SER systems. 
To simulate data scarcity, we select a target low-resource language $\kappa$ and we eliminate all its samples except those in the neutral class, repurposing $\mathcal{D}^{(\kappa)}$ into $\tilde{\mathcal{D}}^{(\kappa)}=\{(x_i, y_i)\mid y_i=\text{``neutral''}\}$. 
% 10\% of the dataset is set as a validation and test sets, respectively, and the rest is used for training. 
All experiments are conducted with 3 folds and averaged. 
The total number of speech samples is BLABLA, and the average duration is BLABLA seconds.\todo{update}

\subsection{Implementation details and evaluation metrics}
We adopt three pretrained foundation models: WavLM-large~\cite{wavlm}, emotion2vec-based, and Whisper-large~\cite{whisper}. 
In the first and second stages, where pre-training the SER model and diffusion model for augmentation, following training datsaet is used: $\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$. 
% The diffusion model is trained with the combination $\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$, i.e., all other languages and neutral examples from the target language. 
The diffusion models are conditioned with emotion embedding, and the sampled neutral utterance embedding $z^{(l)}\in\tilde{\mathcal{D}}^{(\kappa)}$, when the GeLDA is operated in $\mathcal{Z}^{(l)}$ space. 
Then, the diffusion model gerates new feature vectors $\bar{z}$ for every emotions for the target language $\kappa$. The resulting synthetic dataset is denoted as $\bar{\mathcal{D}}^{(\kappa)}$.
% The trained diffusion model generates new feature vectors $\bar{z}$ for all non-neutral emotions to improve balance in $\tilde{\mathcal{D}}^{(\kappa)}$. The resulting synthetic dataset is denoted as $\bar{\mathcal{D}}^{(\kappa)}$. 
Finally, in the fine-tuning stage (the third training stage), the task adapter layers later than $>l$ are updated using neutral Gt examples $\tilde{\mathcal{D}}^{(\kappa)}$ and synthesized examples $\bar{\mathcal{D}}^{(\kappa)}$ that are all in the $\mathcal{Z}^{(l)}$ space. Additional implementation details can be found in Appendix~\ref{apdx:implementation_details}.

Due to the highly imbalanced nature of the emotional speech dataset, SER models are often evaluated with unweighted average (UA) recall, together with weighted average (WA) recall \cite{interspeech_emotion_challenge}. UA metric assesses per-emotion recall first and then averages them to equally account for all the emotions, preventing the domination by the most popular class. Therefore, the UA is often a better indicator of the systemâ€™s accuracy \cite{interspeech_emotion_challenge}. WA is calculated as a top-1 accuracy. 

\subsection{Comparison models} 
For the comparison models, we first build two baseline systems original data (i.e., with no augmentation): 
% $\mathcal{D}^{\setminus \kappa}$ (missing the target language), 
$\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$ (including the neutral class from the target language), and $\mathcal{D}$ (the entire multi-lingual dataset for the \textit{oracle} upper-bound performance). 


As another set of baselines, we use state-of-the-art 
% multilingual zero-shot TTS (XTTS~\cite{xtts}\footnote{\url{https://huggingface.co/coqui/XTTS-v2}}) and 
speech translation systems (SeamlessM4T-Large and Seamless Expressive~\cite{seamless}) to synthesize new speech signals in the input space $\mathcal{X}$, where Seamless Expressive is an version of a Seamless M4T model focusing on the expressivness of the speech.
% , bearing with the limitation in fine-tuning them with our emotional speech dataset. This is due to the missing text scripts in our dataset, which are critical in training TTS systems. 
These systems imitate speaking style and speaker identity of a provided speech prompt. For this, we randomly select English speech with the target emotion serve as prompts.
% XTTS \cite{xtts} is a zero-shot TTS system that can imitate the input speech prompt's speaking style and speaker identity, i.e., $(x_i, y_i)\sim\mathcal{D}^{\setminus \kappa}$. From \cite{seamless}, we adopt SeamlessM4T-Large and SeamlessExpressive models. SeamlessM4T-Large is a speech-to-speech translation model trained with a massive amount of multilingual dataset, and SeamlessExpressive is a version that focuses more on expressiveness by adopting expressive speech datasets. 
These models require over 20 K hours of training data and 450 M parameters, making it difficult to acquire such models for low-resource languages. In contrast, the proposed GeLDA only requires 73 hours of data and 15 M parameters, demonstrating much greater efficiency. Additional efficiency comparison is provided in Appendix~\ref{apdx:efficiency_comparison} Table \ref{tab:compare2}.\todo{touch this paragraph, or move it to results.}
Note that the Seamless Expressive model only supports French, German, Italian, and Spanish in our low-resource languages, and we report the average results of these four languages.
% Note that XTTS does not support Bangla, and SeamlessExpressive does not support Bangla, Korean, and Polish. Therefore, these unsupported languages are excluded when calculating the average performance for the corresponding models.
% each model.

% In addition to the data augmentation in $\mathcal{X}$, we also implement simple linear transformation and noise addition-based DA in $\mathcal{Z}$. Similar to the Latent Filling~\cite{latent_filling} method, we apply interpolation between the existing latent samples that share the same emotions and genders, and Gaussian noise addition. We used the same hyperparameter settings as in \cite{latent_filling}.
% the linear transformation and noise addition data augmentation methods working in $\mathcal{Z}$. Following the methods proposed in \cite{latent_filling}, 


\begin{table*}[t]
\caption{(Zero-shot Results) Average test results for multi-lingual SER models on four languages (French, German, Spanish, Italian). Bold and underlined values indicate the best and second-best performances, respectively. Aug. Space indicates the different locations where DAs are applied.}
    \label{tab:result-main}
    \centering
    \footnotesize
    \resizebox{.9\textwidth}{!}{
    \begin{tabular}{lcrrrrrrr|rrr}
%     \toprule
%     Experiment & Angry & Disgust & Fear & Happy & Neutral & Sad & Surprise & UAR & WA & Macro-F1 & UA without target emotion \\ 
%     \midrule
%     \multicolumn{12}{c}{3 Languages}\\
%     \midrule
% Baseline (pre-trained) & 65.67 & 42.33 & 38.00 & 52.00 & 98.00 & 47.67 & 23.00 & 55.83 & 57.75 & 55.36 & 47.98 \\ 
% ours baseline finetune & 54.33 & 29.00 & 31.67 & 43.33 & 99.67 & 35.67 & 16.00 & 47.88 & 49.69 & 47.31 & 38.27 \\ 
% \midrule
% seamless m4t h0 (x3) & 35.67 & 20.67 & 8.67 & 38.67 & 99.33 & 29.67 & 3.00 & 38.00 & 39.17 & 33.33 & 26.65 \\ 
% seamless expressive h0 (x3) & 51.67 & 25.33 & 36.33 & 47.33 & 97.33 & 36.33 & 23.00 & 48.23 & 49.17 & 47.80 & 39.12 \\ 
% \midrule
% ours h0 (x3) & 75.00 & 43.33 & 54.33 & 64.33 & 77.67 & 62.33 & 59.00 & 62.74 & 64.07 & 64.07 & 60.04 \\ 
% ours h1 (x3)& 65.33 & 49.67 & 51.33 & 60.00 & 46.33 & 61.67 & 43.00 & 54.98 & 55.66 & 55.92 & 56.72 \\ 
% ours h2 (x3)& 69.33 & 52.33 & 45.67 & 54.67 & 68.67 & 55.00 & 33.00 & 56.42 & 57.59 & 57.82 & 54.31 \\ 
% ours h0 (x10)& 74.00 & 41.00 & 55.67 & 65.33 & 74.67 & 63.33 & 56.00 & 62.06 & 63.47 & 62.75 & 59.70 \\ 
% ours h1 (x10)& 61.33 & 52.00 & 43.00 & 58.00 & 46.67 & 55.00 & 46.00 & 52.27 & 52.30 & 53.59 & 53.34 \\ 
% ours h2 (x10)& 64.33 & 56.00 & 40.33 & 54.33 & 65.67 & 59.67 & 34.00 & 55.61 & 56.25 & 57.07 & 53.87 \\ 
% \midrule
% \multicolumn{12}{c}{10 Languages}\\
% \midrule
% Baseline (pre-trained) & 60.50 & 31.17 & 34.22 & 44.56 & 96.20 & 36.22 & 30.75 & 51.05 & 52.54 & 48.16 & 40.25 \\ 
% ours baseline finetune & 47.30 & 22.83 & 28.11 & 33.89 & 99.70 & 26.33 & 25.00 & 43.83 & 45.57 & 40.50 & 30.49 \\ 
% \midrule
% ours h0 (x3)& 70.50 & 34.00 & 48.22 & 59.33 & 73.60 & 40.22 & 59.25 & 54.84 & 55.91 & 54.87 & 51.02 \\ 
% ours h1 (x3)& 67.80 & 38.00 & 44.33 & 58.33 & 45.40 & 45.67 & 55.75 & 49.54 & 49.45 & 49.35 & 50.57 \\ 
% ours h2 (x3)& 70.30 & 38.33 & 41.78 & 54.56 & 62.40 & 46.67 & 51.00 & 52.44 & 53.09 & 52.70 & 50.60 \\ 
% ours h0 (x10)& 69.20 & 29.33 & 48.00 & 59.11 & 73.40 & 41.11 & 60.50 & 54.20 & 55.08 & 53.80 & 50.22 \\ 
% ours h1 (x10)& 66.40 & 39.33 & 40.67 & 55.67 & 42.20 & 42.89 & 55.25 & 47.65 & 47.44 & 48.29 & 48.99 \\ 
% ours h2 (x10)& 68.40 & 40.50 & 39.78 & 55.11 & 59.90 & 47.22 & 52.00 & 51.34 & 51.91 & 51.78 & 49.73 \\ 

    \toprule
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Aug. Space}} & \multicolumn{7}{c}{\textbf{Per emotion recall (\%)}} & 
        \multirow{2}{*}{\textbf{UA (\%)}} & \multirow{2}{*}{\textbf{WA (\%)}} & 
        \multirow{2}{*}{\textbf{Macro-F1 (\%)}}\\ 
        \cline{3-9}
        ~ & ~ &\textbf{Angry} & \textbf{Disgust} & \textbf{Fear} & \textbf{Happy} & \textbf{Neutral} & \textbf{Sad} & \multicolumn{1}{c}{\textbf{Surprise}} & ~ & ~ & ~ \\
        \midrule

\hline
    \rowcolor{gray!20}\multicolumn{12}{c}{Backbone foundation model: WavLM-large} \\
    Pre-trained & -- & 62.75 & 40.25 & 32.00 & 44.00 & 92.50 & 41.00 & 29.50 & 50.69 & 50.98 & 49.63\\ 
    Fine-tuned & -- & 45.75 & 23.50 & 21.00 & 27.75 & 99.50 & 31.00 & 15.00 & 39.92 & 40.17 & 37.08 \\ 
    \hline
Seamless M4T & $\mathcal{X}$ & 32.50 & 15.50 & 10.75 & 26.00 & 96.00 & 33.00 & 17.50 & 34.68 & 34.90 & 30.68\\ 
Seamless Expressive & $\mathcal{X}$ & 45.75 & 20.75 & 22.00 & 34.75 & 91.00 & 46.00 & 33.00 & 42.79 & 42.94 & 40.64 \\ 
\hline
~& $\mathcal{Z}^{(0)}$ & 68.50 & 47.00 & 42.75 & 43.00 & 64.75 & 51.75 & 47.50 & 52.62 & 53.34 & 52.11\\ 
GeLDA (Ours) & $\mathcal{Z}^{(1)}$ & 65.00 & 43.75 & 47.75 & 44.75 & 53.25 & 47.00 & 42.50 & 49.57 & 51.02 & 48.81\\ 
~& $\mathcal{Z}^{(2)}$ & 66.75 & 46.50 & 34.25 & 52.50 & 59.50 & 55.00 & 45.00 & 51.78 & 52.06 & 50.64\\ 
    \hline
    \rowcolor{gray!20}\multicolumn{12}{c}{Backbone foundation model: emotion2vec-base} \\
    Pre-trained & -- & 50.50 & 31.25 & 24.50 & 25.25 & 90.00 & 43.25 & 20.00 & 42.44 & 42.20 & 39.78 \\ 
Fine-tuned & -- & 10.75 & 6.25 & 8.00 & 4.50 & 99.75 & 7.25 & 7.00 & 21.77 & 21.27 & 14.05 \\ 
\hline
Seamless M4T & $\mathcal{X}$ & 18.25 & 12.50 & 9.25 & 6.75 & 94.00 & 20.25 & 6.50 & 25.50 & 25.30 & 19.54\\ 
Seamless Expressive & $\mathcal{X}$ & 32.75 & 16.75 & 24.50 & 17.50 & 92.25 & 41.00 & 13.50 & 36.00 & 35.78 & 32.82\\ 
\hline
~& $\mathcal{Z}^{(0)}$ & 44.25 & 47.25 & 37.25 & 31.00 & 78.25 & 44.25 & 29.00 & 46.02 & 45.06 & 44.38 \\ 
GeLDA (Ours) & $\mathcal{Z}^{(1)}$ & 47.25 & 38.00 & 33.25 & 26.00 & 54.50 & 43.25 & 34.50 & 39.92 & 39.73 & 38.39 \\ 
~& $\mathcal{Z}^{(2)}$ & 58.50 & 44.50 & 32.75 & 28.75 & 55.75 & 45.00 & 28.50 & 43.12 & 42.95 & 41.86\\ 
\hline
    \rowcolor{gray!20}\multicolumn{12}{c}{Backbone foundation model: Whisper-large} \\
    Pre-trained & -- & 66.00 & 45.50 & 41.50 & 53.50 & 95.75 & 50.00 & 43.00 & 57.65 & 58.88 & 57.09\\ 
    Fine-tuned & -- & 56.50 & 33.50 & 35.75 & 43.75 & 99.25 & 38.00 & 33.50 & 50.10 & 51.03 & 49.62\\ 
    \hline
Seamless M4T & $\mathcal{X}$ & 39.25 & 22.25 & 13.50 & 38.00 & 97.25 & 36.50 & 25.50 & 40.41 & 41.24 & 37.88 \\ 
Seamless Expressive & $\mathcal{X}$ & 49.75 & 29.75 & 32.25 & 46.25 & 94.25 & 41.75 & 49.00 & 49.08 & 49.34 & 48.72\\ 
\hline
~ & $\mathcal{Z}^{(0)}$ & 75.50 & 51.25 & 59.75 & 64.00 & 77.50 & 54.75 & 57.50 & 63.58 & 64.49 & 63.33\\ 
GeLDA (Ours) & $\mathcal{Z}^{(1)}$ & 69.50 & 53.50 & 44.00 & 62.00 & 62.25 & 60.00 & 54.50 & 58.17 & 59.10 & 57.62 \\ 
~& $\mathcal{Z}^{(2)}$& 70.50 & 56.00 & 47.50 & 59.50 & 73.25 & 55.50 & 51.50 & 59.85 & 60.70 & 59.44 \\ 
\bottomrule
    \end{tabular}
}
\end{table*}

% \begin{table*}[t]
% \caption{Average test results for multi-lingual SER models on six low-resource target languages. A star marks models evaluated only on supported languages. Bold and underlined values indicate the best and second-best performances, respectively. DA indicates the different locations where DAs are applied. Results with the Whisper-small backbone can be found in Appendix~\ref{apdx:additional-whisper-small}.}
%     \label{tab:result-main}
%     \centering
%     \footnotesize
%     \resizebox{.9\textwidth}{!}{
%     \begin{tabular}{lrrrrrrr|rrr}
%     \toprule
%     Experiment & Angry & Disgust & Fear & Happy & Neutral & Sad & Surprise & UAR & WA & Macro-F1 \\ 
%     \midrule
%     \multicolumn{11}{c}{3 Languages}\\
%     \midrule
% Baseline (pre-trained) & 86.00 & 73.00 & 69.00 & 76.00 & 91.00 & 76.00 & 71.00 & 77.99 & 78.48 & 78.31 \\ 
% ours baseline finetune & 87.27 & 75.98 & 68.93 & 77.69 & 91.43 & 78.34 & 71.67 & 79.67 & 79.96 & 79.82 \\ 
% \midrule
% seamless m4t h0 (x3) & 82.78 & 74.11 & 64.22 & 79.00 & 88.33 & 76.67 & 73.33 & 77.44 & 78.04 & 77.51 \\ 
% + Bootstrapping & 87.89 & 80.33 & 72.89 & 80.78 & 90.11 & 77.56 & 68.00 & 81.15 & 81.18 & 80.93 \\ 
% seamless expressive h0 (x3) & 83.89 & 70.11 & 67.00 & 77.67 & 89.89 & 76.00 & 70.33 & 77.21 & 78.10 & 77.09 \\ 
% + Bootstrapping & 84.22 & 72.11 & 74.22 & 80.78 & 89.00 & 76.44 & 71.67 & 79.29 & 79.87 & 79.50 \\ 
% \midrule
% Latent Filling h0 & 88.44 & 78.11 & 72.33 & 83.22 & 92.11 & 81.56 & 73.33 & 82.37 & 82.67 & 82.55 \\ 
% Latent Filling h1 & 88.78 & 74.22 & 74.44 & 82.00 & 90.89 & 78.67 & 70.67 & 81.16 & 81.52 & 81.37 \\ 
% Latent Filling h2 & 88.22 & 73.11 & 71.11 & 77.78 & 92.00 & 79.56 & 73.00 & 80.13 & 80.57 & 80.33 \\ 
% \midrule
% ours h0 (x3) & 85.84 & 73.19 & 74.52 & 83.31 & 93.19 & 77.05 & 72.56 & 80.89 & 81.36 & 80.90 \\ 
% + Bootstrapping & 87.88 & 76.15 & 74.93 & 83.94 & 92.17 & 84.68 & 70.64 & 82.90 & 83.06 & 82.89 \\ 
% ours h1 (x3) & 84.79 & 75.12 & 70.24 & 79.58 & 88.48 & 75.27 & 67.31 & 78.52 & 79.00 & 78.67 \\ 
% + Bootstrapping & 87.31 & 76.74 & 73.32 & 78.67 & 90.50 & 78.99 & 73.97 & 80.76 & 81.13 & 80.88 \\ 
% ours h2 (x3) & 85.67 & 73.96 & 68.61 & 76.33 & 89.27 & 77.31 & 68.33 & 78.20 & 78.60 & 78.21 \\ 
% + Bootstrapping & 86.67 & 74.52 & 69.76 & 78.15 & 89.95 & 78.17 & 69.49 & 79.20 & 79.60 & 79.26 \\ 
% \midrule
% ours h0 (x10) & 87.39 & 77.94 & 75.72 & 85.25 & 93.15 & 79.21 & 71.67 & 82.77 & 83.07 & 82.78 \\ 
% + Bootstrapping  & 87.08 & 79.70 & 78.81 & 83.68 & 92.23 & 81.30 & 67.44 & 83.24 & 83.45 & 83.24 \\ 
% ours h1 (x10) & 83.92 & 74.51 & 70.27 & 81.85 & 88.78 & 77.08 & 69.62 & 79.15 & 79.50 & 79.23 \\ 
% + Bootstrapping  & 83.73 & 76.73 & 73.54 & 84.76 & 92.89 & 79.39 & 71.67 & 81.56 & 81.75 & 81.61 \\ 
% ours h2 (x10) & 86.37 & 74.93 & 69.55 & 79.55 & 88.08 & 77.56 & 69.62 & 79.05 & 79.36 & 79.03 \\ 
% + Bootstrapping & 86.31 & 72.85 & 69.28 & 76.98 & 90.10 & 78.44 & 70.26 & 78.72 & 79.15 & 78.73 \\ 

% \midrule
% \multicolumn{11}{c}{12 Languages}\\
% \midrule
% Baseline (pre-trained) & 88.00 & 66.00 & 68.00 & 79.00 & 81.00 & 75.00 & 72.00 & 76.93 & 77.84 & 76.46 \\ 
% ours baseline finetune & 88.30 & 69.03 & 66.96 & 79.12 & 80.87 & 76.22 & 71.84 & 77.29 & 78.29 & 76.75 \\ 
% \midrule
% Latent Filling h0 & 89.64 & 71.67 & 66.10 & 82.55 & 83.17 & 77.73 & 75.75 & 79.39 & 80.57 & 78.77 \\ 
% Latent Filling h1 & 89.11 & 70.00 & 70.50 & 81.27 & 81.27 & 76.27 & 75.25 & 78.67 & 79.65 & 78.10 \\ 
% Latent Filling h2 & 89.03 & 67.62 & 69.93 & 80.30 & 81.20 & 77.76 & 74.33 & 78.35 & 79.25 & 77.73 \\ 
% \midrule
% ours h0 (x3) & 88.58 & 68.75 & 73.32 & 82.92 & 84.36 & 79.06 & 71.92 & 80.02 & 80.39 & 78.92 \\ 
% + Bootstrapping  & 90.40 & 70.75 & 73.95 & 84.07 & 82.62 & 80.78 & 70.62 & 80.88 & 81.16 & 79.58 \\ 
% ours h1 (x3) & 85.53 & 69.56 & 72.48 & 80.63 & 80.21 & 75.98 & 73.10 & 78.00 & 78.21 & 76.73 \\ 
% + Bootstrapping & 87.00 & 70.25 & 72.78 & 80.50 & 80.87 & 77.87 & 75.90 & 78.78 & 79.10 & 77.68 \\ 
% ours h2 (x3)  & 87.82 & 67.57 & 68.28 & 78.68 & 80.42 & 75.07 & 72.49 & 77.15 & 77.91 & 76.25 \\ 
% + Bootstrapping & 88.43 & 68.14 & 68.60 & 79.52 & 80.89 & 76.25 & 73.17 & 77.70 & 78.47 & 76.84 \\ 
% \midrule
% ours h0 (x10) & 88.35 & 71.54 & 73.75 & 84.60 & 83.49 & 79.62 & 71.80 & 80.65 & 81.01 & 79.52 \\ 
% + Bootstrapping & 87.98 & 73.68 & 73.39 & 84.83 & 83.32 & 81.09 & 70.32 & 81.01 & 81.42 & 79.77 \\ 
% ours h1 (x10) & 85.98 & 68.50 & 71.79 & 82.56 & 81.64 & 78.17 & 73.18 & 78.86 & 79.13 & 77.62 \\ 
% + Bootstrapping & 87.64 & 71.10 & 73.38 & 83.74 & 83.31 & 79.08 & 75.21 & 80.20 & 80.38 & 78.89 \\ 
% ours h2 (x10) & 87.70 & 66.49 & 67.91 & 79.34 & 80.68 & 76.36 & 71.90 & 77.35 & 78.04 & 76.31 \\ 
% + Bootstrapping & 87.75 & 67.29 & 68.50 & 79.92 & 80.90 & 77.04 & 73.26 & 77.70 & 78.39 & 76.77 \\ 
% \bottomrule
%     \end{tabular}
%     }
% \end{table*}

\iffalse
\begin{table}[t]
    \caption{Average test results for multi-lingual SER models on six low-resource target languages. A star marks models evaluated only on supported languages. Bold and underlined values indicate the best and second-best performances, respectively. DA indicates the different locations where DAs are applied. Results with the Whisper-small backbone can be found in Appendix~\ref{apdx:additional-whisper-small}.}
    \label{tab:result-main}
    \centering
    \footnotesize
    \resizebox{.9\textwidth}{!}{
    \begin{tabular}{lccccccrr}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{DA}} & \multicolumn{5}{c}{\textbf{Per emotion recall (\%)}} & 
        \multirow{2}{*}{\textbf{UA (\%)}} & \multirow{2}{*}{\textbf{WA (\%)}} \\ 
        \cline{3-7}
        ~ & ~ &\textbf{Angry} & \textbf{Happy} & \textbf{Neutral} & \textbf{Sad} & \multicolumn{1}{c}{\textbf{Surprise}} & ~ & ~\\
        \midrule
        \rowcolor{gray!20}\multicolumn{9}{c}{Backbone foundation model: XLSR} \\
        Baseline ($\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$) & - & 41.97 & 41.80 & 98.00 & 35.00 & 20.10 & 47.37 & 65.07 \\
        Baseline ($\mathcal{D}^{\setminus \kappa}$) & - & 61.79 & 51.78 & 60.60 & 58.06 & 24.57 & 51.36 & 59.27\\ 
        \hline
        XTTS* & ~ &47.48 & 44.64 & 87.78 & 35.54 & 24.52 & 47.99 
 & \textbf{64.89}\\ 
        Seamless M4T v2 & $\mathcal{X}$ & 45.77 & 39.60 & 90.29 & 37.99 & 20.18 & 46.76 & 62.13\\ 
        Seamless Expressive* & ~ & 46.98 & 50.90 & 82.55 & 47.55 & 17.56 & 49.11 & 60.62\\
        \hline
        ~ & $\mathcal{Z}^{(0)}$ & 54.24 & 56.55 & 75.77 & 52.6 & 23.51 & 52.54 & 63.44 \\ 
        Latent Filling & $\mathcal{Z}^{(1)}$ &55.48 & 58.55 & 61.49 & 53.36 & 26.08 & 50.99 & 59.91\\ 
        ~ & $\mathcal{Z}^{(2)}$ & 59.31 & 57.43 & 60.17 & 55.48 & 24.57 & 51.39 & 59.64 \\ 
        \hline
        ~ & $\mathcal{Z}^{(0)}$ & 51.37 & 44.81 & 90.98 & 41.07 & 23.68 & 50.38 & \underline{64.69}\\ 
        GeLDA (ours) & $\mathcal{Z}^{(1)}$ & 60.71 & 58.75 & 56.00 & 57.22 & 35.83 & \textbf{53.70} & 59.62\\ 
        ~ & $\mathcal{Z}^{(2)}$ & 60.51 & 56.95 & 61.22 & 57.87 & 28.33 & \underline{52.97} & 59.93\\ 
        % ~ & Proposed - DDIM & 44.79 & 39.42 & 96.94 & 50.52 & 57.27 & 57.79 & 57.62 \\ 
        % ~ & Proposed - DDPM & 46.88 & 46.15 & 97.96 & 44.33 & 52.73 & 57.61 & 57.43 \\ 
        % ~ & Proposed - DDIM CFG++ & 48.96 & 50.96 & 92.86 & 50.52 & 62.73 & 61.20 & 61.19 \\ 
        % ~ & Proposed - DDIM CFG++ (x5) & 50.00 & 52.88 & 92.86 & 53.61 & 70.00 & 63.87 & 63.96 \\ 
        \hline
        Baseline ($\mathcal{D}$)& - & 75.52 & 59.97 & 79.20 & 68.83 & 38.66 & 64.43 & 74.19\\ 
        % ~ & + Fine-tuned (All) &  \\ 
        
        \midrule
        % \rowcolor{gray!20}\multicolumn{9}{c}{Backbone foundation model: Whisper-small} \\
        % Baseline ($\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$) & - & 35.70 & 32.32 & 99.71 & 23.65 & 16.06 & 41.48 & 60.85 \\
        % Baseline ($\mathcal{D}^{\setminus \kappa}$)  & - &61.26 & 59.77 & 65.54 & 58.29 & 27.48 & 54.47 & 63.35\\
        % \hline
        % XTTS* & ~ & 41.57 & 35.15 & 95.36 & 31.76 & 17.81 & 44.33 & \underline{65.89}\\
        % Seamless M4T v2 & $\mathcal{X}$ & 42.12 & 34.23 & 94.50 & 28.70 & 16.23 & 43.16 & 60.98\\
        % Seamless Expressive* & ~ & 59.03 & 45.91 & 83.45 & 51.67 & 28.89 & 53.79 & 64.57\\
        % \hline
        % ~ & $\mathcal{Z}^{(0)}$ & 45.31 & 45.16 & 94.85 & 37.95 & 19.78 & 48.61 & 64.45\\
        % Latent Filling & $\mathcal{Z}^{(1)}$ & 55.39 & 56.7 & 82.65 & 53.16 & 24.84 & 54.55 & \textbf{67.09}\\
        % ~ & $\mathcal{Z}^{(2)}$ & 60.57 & 59.15 & 69.44 & 57.41 & 27.24 & 54.76 & 63.89\\
        % \hline
        % ~ & $\mathcal{Z}^{(0)}$ & 44.79 & 34.34 & 95.17 & 29.70 & 21.64 & 45.12 & 63.60\\
        % Proposed &$\mathcal{Z}^{(1)}$& 58.02 & 44.54 & 79.64 & 59.07 & 33.32 & \underline{54.92} & 65.84\\
        % ~ & $\mathcal{Z}^{(2)}$& 60.92 & 57.95 & 69.49 & 61.38 & 35.74 & \textbf{57.09} & 64.24\\
        % \hline
        % Baseline ($\mathcal{D}$)& - & 77.22 & 68.43 & 86.06 & 69.38 & 36.28 & 67.47 & 78.91\\
        % \midrule

        \rowcolor{gray!20}\multicolumn{9}{c}{Backbone foundation model: Whisper-large} \\
        Baseline ($\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$) & - & 45.01 & 39.61 & 98.97 & 36.25 & 24.81 & 48.93 & 66.82\\
        Baseline ($\mathcal{D}^{\setminus \kappa}$)  & - &70.39 & 67.98 & 68.28 & 74.08 & 34.16 & 62.98 & 68.96\\
        \hline
        XTTS* & ~ & 41.87 & 43.88 & 93.94 & 41.19 & 26.95 & 49.57 & 68.91 \\
        Seamless M4T v2 & $\mathcal{X}$ & 51.02 & 49.61 & 94.36 & 40.40 & 29.04 & 52.89 & 69.60 \\
        Seamless Expressive* & ~ & 57.05 & 57.32 & 88.53 & 51.33 & 32.79 & 57.40 & 68.60 \\
        \hline
        ~ & $\mathcal{Z}^{(0)}$& 59.39 & 58.23 & 93.13 & 56.85 & 33.11 & 60.14 & \textbf{73.56} \\
        Latent Filling & $\mathcal{Z}^{(1)}$ & 69.06 & 68.03 & 78.00 & 70.92 & 34.05 & 64.01 & \underline{73.35} \\
        ~ & $\mathcal{Z}^{(2)}$& 70.14 & 68.64 & 69.69 & 73.73 & 34.35 & 63.31 & 69.57\\
        \hline
        ~ & $\mathcal{Z}^{(0)}$ & 54.91 & 51.25 & 96.13 & 41.79 & 28.75 & 54.57 & 71.18\\
        GeLDA (ours) & $\mathcal{Z}^{(1)}$ & 72.12 & 63.07 & 79.17 & 70.01 & 43.09 & \textbf{65.49} & 72.36 \\
        ~ & $\mathcal{Z}^{(2)}$  & 69.96 & 65.85 & 70.23 & 72.93 & 41.56 & \underline{64.11} & 69.05\\
        \hline
        ~ & $\mathcal{Z}^{(0)}$ & 72.21 & 66.33 & 84.59 & 64.82 & 36.56 & 64.90 & \textbf{73.79}\\
        GeLDA $\times 20$ (ours) & $\mathcal{Z}^{(1)}$  & 71.78 & 68.16 & 70.12 & 74.62 & 48.72 & \textbf{66.68} & 70.78\\
        ~ & $\mathcal{Z}^{(2)}$ & 69.97 & 67.43 & 69.61 & 71.86 & 42.95 & 64.36 & 69.16\\
        
        % ~ & Proposed - DDIM CFG++  & 41.67 & 56.73 & 92.86 & 65.98 & 68.18 & 65.08 & 65.15 \\
        \hline
        Baseline ($\mathcal{D}$) & - & 83.53 & 74.22 & 84.98 & 76.54 & 44.72 & 72.80 & 81.41\\
        % ~ & + Finetuned (All) & 84.38 & 76.50 & 80.80 & 79.94 & 56.57 & 75.64 & 79.66\\
        
        \bottomrule
    \end{tabular}
    }
\end{table}
\fi


\subsection{Results}
Table~\ref{tab:result-main} presents the results for three different backbone foundation models: WavLM-large, emotion2vec-base, and Whisper-large. \todo{this may move to the comparison models part.} Since the pre-trained model was trained on various languages even only with the neutral emotion data for the target language, it could somewhat perform for other emotions. This is because the model learned the language shared characteristic of emotion expression. After fine-tuning, the model forgets the language shared emotion characteristic and highly overfitted to the neutral emotion. This is because for the target language, we only have data with neutral emotion.
% The baselines trained with $\mathcal{D}^{\setminus \kappa}\cup \tilde{\mathcal{D}}^{(\kappa)}$ perform the worst in terms of UA due to the bias toward neutral emotion in the target language $\tilde{\mathcal{D}}^{(\kappa)}$. $\mathcal{D}^{\setminus \kappa}$ represents a zero-shot condition, i.e., language $\kappa$ is missing entirely. 

Applying input space DA does not bring performance improvement. We attribute this to the limited emotional expressiveness of synthetic speech. SeamlessExpressive achieves better results than the Seamless M4T, highlighting the importance of expressive generation. However, all TTS models fail to improve the class imbalance (e.g., poor accuracy in \textit{``disgust''} and \textit{``fear''}).
% Simple latent DA, Latent Filling, also fails to improve upon the zero-shot baseline ($\mathcal{D}^{\setminus \kappa}$), although it outperforms TTS-based DA, implying a well-defined latent space is effective for DA.

\textbf{This paragraph not yet updated.}
With our proposed GeLDA method, UA improves by 5.93\%, 3.58\%, and 1.93\% points for Whisper-large, emotion2vec-base, and WavLM-large, respectively. Given the upper bound improvement by using oracle data $\mathcal{D}$ is 9.82\%\todo{update} for the Whisper-large backbone model, these improvements are substantial. Among different latent spaces, augmentation at $\mathcal{Z}^{(1)}$ consistently outperforms others, revealing the tradeoff in choosing the right latent space.
Figure~\ref{fig:result-embedding-bangla} shows t-SNE plots of ground-truth (GT) test embeddings and generated vectors in Bangla. Augmentation at $\mathcal{Z}^{(0)}$ produces samples far from the GT distribution, likely due to the irrelevant information in the low-level feature space. However, augmented data at $\mathcal{Z}^{(1)}$ and $\mathcal{Z}^{(2)}$ show better harmonization with the GT embeddings.
Comparing $\mathcal{Z}^{(1)}$ and $\mathcal{Z}^{(2)}$, although GeLDA appears to be more successful in $\mathcal{Z}^{(2)}$, it limits the following fine-tuning process's capability. 

\textbf{This paragraph not yet updated.}
In summary, the proposed GeLDA method can significantly improve the language-specific SER results by using augmented data with a matching language label.  More detailed results on each language can be found in Appendix \ref{apdx:each_language}. Additional SER results with Whisper-small, visualization of the augmented features before and after fine-tuning, and language-agnostic GeLDA for multilingual SER are also presented in \ref{app:additional_ser}.

\iffalse
\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.28\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/uar_iter.png}
  \end{minipage}
  % \hspace{0.03\linewidth}
  % \begin{minipage}[b]{0.24\linewidth}
  %   \centering
  %   \includegraphics[width=\linewidth]{figs/WA_iter.png}
  % \end{minipage}
  % \hspace{0.03\linewidth}
  \begin{minipage}[b]{0.28\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/neutral_iter.png}
  \end{minipage}
  % \hspace{0.03\linewidth}
  \begin{minipage}[b]{0.40\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/others_iter.png}
  \end{minipage}
  \caption{Trend graph as the number of fine-tuning iterations increases with different augmented latent vector locations. (Left) UA, (middle) accuracy of neutral emotion, (right) average accuracy of non-neutral emotions. }
  \label{fig:result-iter}
\end{figure}
\fi

% \vspace{-10pt}



  % (Left) UA, (middle) accuracy on neutral emotion, and (right) average accuracy on non-neutral emotions.


\begin{figure}[ht]
  \centering
  % \vspace{-0.35in}
  \includegraphics[width=\linewidth]{fig/graph_whisper-large-v3_combined_4langs.pdf}
  % \vspace{-0.07in}
  \caption{Graph of the GeLDA performance depending on the number of augmented samples. This is the result of the backbone foundation model of Whipser-large}
  \label{fig:aug_num}
\end{figure}



\begin{figure}[ht]
  \centering
  % \vspace{-0.35in}
  \includegraphics[width=\linewidth]{fig/size_plot_all_metrics_vs_model_size_annot.pdf}
  % \vspace{-0.07in}
  \caption{Graph of the GeLDA performance depending on the size of the diffusion models. This is the result of the backbone foundation model of Whisper-large and a number of augmentations of 200.}
  \label{fig:diffusion_size}
\end{figure}


\subsection{Ablation Study}
% In Figure~\ref{fig:aug_num}, we report the results for the different numbers of augmented samples for each emotion for the target language. The performances of the input space data augmentation methods decrease as the number of augmeted samples increase. This indicate that the quality of generated samples are low.  For the $\mathcal{Z}^{(0)}$, the performance gradually increases as the number of augmented data points increases. However, for the $\mathcal{Z}^{(1)}$ space, the performance gradually decreases, which indicates the quality of the generated samples are bad. We argue that this is because the $\mathcal{Z}^{(1)}$ space contains less information about the languages than the $\mathcal{Z}^{(0)}$, which makes the generated data in $\mathcal{Z}^{(1)}$ space more lower quality than the $\mathcal{Z}^{(0)}$ space. $\mathcal{Z}^{(2)}$ is the very last layer, which makes its latent space linearly seperable, and therefore eliminating all the language specific information. This makes the data augmentation in this space much easier, but also not very effective compared to the $\mathcal{Z}^{(0)}$.

% This trend can be also find in Figure~\ref{fig:diffusion_size} where we conduct the experiments with controling the diffusion model size. For the smallest size model, $\mathcal{Z}^{(1)}$ and $\mathcal{Z}^{(2)}$ perform better than the $\mathcal{Z}^{(0)}$. This may because the information in those spaces are large, and as the diffusion model size increases, it overfitted to the data. In contrast, for the $\mathcal{Z}^{(0)}$, since this is the representation from the foundation model, it contains rich information in there, and as the diffusion model size increases, the performance also gradually increases. We finally highlight that compared to the input space data augmentation methods, our model performes much better with significiantly smaller amount of model size. 

    


In Figure~\ref{fig:aug_num}, we report the results for different numbers of augmented samples per emotion in the target language. The performance of input-space data augmentation methods decreases as the number of augmented samples increases, indicating that the quality of the generated samples is low. For the $\mathcal{Z}^{(0)}$ space, performance gradually improves as the number of augmented data points increases. In contrast, performance in the $\mathcal{Z}^{(1)}$ space gradually degrades, suggesting that the generated samples in this space are of poor quality. We attribute this to the fact that $\mathcal{Z}^{(1)}$ contains less language-related information than $\mathcal{Z}^{(0)}$, resulting in lower-quality generated data. The $\mathcal{Z}^{(2)}$ space corresponds to the final layer, whose latent representations are linearly separable and thus largely eliminate language-specific information. 
% Although generation in $\mathcal{Z}^{(2)}$ is easier due to linear separability, the lack of semantic diversity limits the usefulness of additional augmented samples.
This property makes data augmentation in this space easier, but it is less effective compared to augmentation in $\mathcal{Z}^{(0)}$.

A similar trend is observed in Figure~\ref{fig:diffusion_size}, where we analyze the effect of diffusion model size. For the smallest diffusion model, $\mathcal{Z}^{(1)}$ and $\mathcal{Z}^{(2)}$ outperform $\mathcal{Z}^{(0)}$. This may be because these spaces are lower-dimensional and contain less information, making them easier to model with small diffusion models; however, as model size increases, they become prone to overfitting. In contrast, $\mathcal{Z}^{(0)}$, which is derived from the foundation model, contains richer information. As a result, increasing the diffusion model size consistently improves performance in this space.
Finally, we emphasize that compared to input-space data augmentation methods, which require larger models to capture surface-level variability, our approach achieves significantly better performance while requiring a substantially smaller model size.